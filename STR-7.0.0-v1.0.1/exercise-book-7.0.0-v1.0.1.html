<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.15">
<title>Confluent Stream Processing using Apache Kafka® Streams &amp; ksqlDB: Exercise Book</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment @import statement to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;word-wrap:anywhere;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede;word-wrap:normal}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre).nobreak{word-wrap:normal}
:not(pre).nowrap{white-space:nowrap}
:not(pre).pre-wrap{white-space:pre-wrap}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:none;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details>summary:first-of-type{cursor:pointer;display:list-item;outline:none;margin-bottom:.75em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6);word-wrap:anywhere}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{-webkit-border-radius:4px;border-radius:4px;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class="highlight"],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos{border-right:1px solid currentColor;opacity:.35;padding-right:.5em}
pre.pygments .lineno{border-right:1px solid currentColor;opacity:.35;display:inline-block;margin-right:.75em}
pre.pygments .lineno::before{content:"";margin-right:-.125em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;font-size:.85rem;text-align:left;margin-right:0}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content{margin-bottom:1.25em;word-wrap:anywhere}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>*>tr>*{border-width:1px}
table.grid-cols>*>tr>*{border-width:0 1px}
table.grid-rows>*>tr>*{border-width:1px 0}
table.frame-all{border-width:1px}
table.frame-ends{border-width:1px 0}
table.frame-sides{border-width:0 1px}
table.frame-none>colgroup+*>:first-child>*,table.frame-sides>colgroup+*>:first-child>*{border-top-width:0}
table.frame-none>:last-child>:last-child>*,table.frame-sides>:last-child>:last-child>*{border-bottom-width:0}
table.frame-none>*>tr>:first-child,table.frame-ends>*>tr>:first-child{border-left-width:0}
table.frame-none>*>tr>:last-child,table.frame-ends>*>tr>:last-child{border-right-width:0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd),table.stripes-even tr:nth-of-type(even),table.stripes-hover tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
td.hdlist2{word-wrap:anywhere}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:50%;border-radius:50%;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#header,#content,#footnotes,#footer{max-width:none}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
<style>
pre.rouge table td { padding: 5px; }
pre.rouge table pre { margin: 0; }
pre.rouge .cm {
  color: #999988;
  font-style: italic;
}
pre.rouge .cp {
  color: #999999;
  font-weight: bold;
}
pre.rouge .c1 {
  color: #999988;
  font-style: italic;
}
pre.rouge .cs {
  color: #999999;
  font-weight: bold;
  font-style: italic;
}
pre.rouge .c, pre.rouge .ch, pre.rouge .cd, pre.rouge .cpf {
  color: #999988;
  font-style: italic;
}
pre.rouge .err {
  color: #a61717;
  background-color: #e3d2d2;
}
pre.rouge .gd {
  color: #000000;
  background-color: #ffdddd;
}
pre.rouge .ge {
  color: #000000;
  font-style: italic;
}
pre.rouge .gr {
  color: #aa0000;
}
pre.rouge .gh {
  color: #999999;
}
pre.rouge .gi {
  color: #000000;
  background-color: #ddffdd;
}
pre.rouge .go {
  color: #888888;
}
pre.rouge .gp {
  color: #555555;
}
pre.rouge .gs {
  font-weight: bold;
}
pre.rouge .gu {
  color: #aaaaaa;
}
pre.rouge .gt {
  color: #aa0000;
}
pre.rouge .kc {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kd {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kn {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kp {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kr {
  color: #000000;
  font-weight: bold;
}
pre.rouge .kt {
  color: #445588;
  font-weight: bold;
}
pre.rouge .k, pre.rouge .kv {
  color: #000000;
  font-weight: bold;
}
pre.rouge .mf {
  color: #009999;
}
pre.rouge .mh {
  color: #009999;
}
pre.rouge .il {
  color: #009999;
}
pre.rouge .mi {
  color: #009999;
}
pre.rouge .mo {
  color: #009999;
}
pre.rouge .m, pre.rouge .mb, pre.rouge .mx {
  color: #009999;
}
pre.rouge .sa {
  color: #000000;
  font-weight: bold;
}
pre.rouge .sb {
  color: #d14;
}
pre.rouge .sc {
  color: #d14;
}
pre.rouge .sd {
  color: #d14;
}
pre.rouge .s2 {
  color: #d14;
}
pre.rouge .se {
  color: #d14;
}
pre.rouge .sh {
  color: #d14;
}
pre.rouge .si {
  color: #d14;
}
pre.rouge .sx {
  color: #d14;
}
pre.rouge .sr {
  color: #009926;
}
pre.rouge .s1 {
  color: #d14;
}
pre.rouge .ss {
  color: #990073;
}
pre.rouge .s, pre.rouge .dl {
  color: #d14;
}
pre.rouge .na {
  color: #008080;
}
pre.rouge .bp {
  color: #999999;
}
pre.rouge .nb {
  color: #0086B3;
}
pre.rouge .nc {
  color: #445588;
  font-weight: bold;
}
pre.rouge .no {
  color: #008080;
}
pre.rouge .nd {
  color: #3c5d5d;
  font-weight: bold;
}
pre.rouge .ni {
  color: #800080;
}
pre.rouge .ne {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nf, pre.rouge .fm {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nl {
  color: #990000;
  font-weight: bold;
}
pre.rouge .nn {
  color: #555555;
}
pre.rouge .nt {
  color: #000080;
}
pre.rouge .vc {
  color: #008080;
}
pre.rouge .vg {
  color: #008080;
}
pre.rouge .vi {
  color: #008080;
}
pre.rouge .nv, pre.rouge .vm {
  color: #008080;
}
pre.rouge .ow {
  color: #000000;
  font-weight: bold;
}
pre.rouge .o {
  color: #000000;
  font-weight: bold;
}
pre.rouge .w {
  color: #bbbbbb;
}
pre.rouge {
  background-color: #f8f8f8;
}
</style>
</head>
<body class="book toc2 toc-left">
<div id="header">
<h1>Confluent Stream Processing using Apache Kafka® Streams &amp; ksqlDB: Exercise Book</h1>
<div class="details">
<span id="revnumber">version 7.0.0-v1.0.1</span>
</div>
<div id="toc" class="toc2">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#_copyright_trademarks">Copyright &amp; Trademarks</a></li>
<li><a href="#_lab_00_introduction">Lab 00 Introduction</a>
<ul class="sectlevel2">
<li><a href="#_a_introduction">a. Introduction</a></li>
<li><a href="#_b_continued_learning_after_class">b. Continued Learning After Class</a></li>
</ul>
</li>
<li><a href="#_lab_01_introduction_to_kafka_streams">Lab 01 Introduction to Kafka Streams</a>
<ul class="sectlevel2">
<li><a href="#_a_scaling_a_kafka_streams_application">a. Scaling a Kafka Streams Application</a></li>
</ul>
</li>
<li><a href="#_lab_02_working_with_kafka_streams">Lab 02 Working with Kafka Streams</a>
<ul class="sectlevel2">
<li><a href="#_a_anatomy_of_a_kafka_streams_app">a. Anatomy of a Kafka Streams App</a></li>
<li><a href="#_b_working_with_json">b. Working with JSON</a></li>
</ul>
</li>
<li><a href="#_lab_03_introduction_to_ksqldb">Lab 03 Introduction to ksqlDB</a>
<ul class="sectlevel2">
<li><a href="#_a_introduction_to_ksqldb">a. Introduction to ksqlDB</a></li>
<li><a href="#_b_using_the_ksqldb_rest_api">b. Using the ksqlDB REST API</a></li>
<li><a href="#_c_creating_connectors_with_ksqldb">c. Creating connectors with ksqlDB</a></li>
</ul>
</li>
<li><a href="#_lab_04_using_ksqldb">Lab 04 Using ksqlDB</a>
<ul class="sectlevel2">
<li><a href="#_a_using_ksqldb">a. Using ksqlDB</a></li>
</ul>
</li>
<li><a href="#_lab_06_windowing_aggregations">Lab 06 Windowing &amp; Aggregations</a>
<ul class="sectlevel2">
<li><a href="#_a_windowing_aggregations">a. Windowing &amp; Aggregations</a></li>
</ul>
</li>
<li><a href="#_lab_07_joins">Lab 07 Joins</a>
<ul class="sectlevel2">
<li><a href="#_a_joining_two_streams">a. Joining Two Streams</a></li>
</ul>
</li>
<li><a href="#_lab_08_custom_processing">Lab 08 Custom Processing</a>
<ul class="sectlevel2">
<li><a href="#_a_using_the_processor_api">a. Using the Processor API</a></li>
</ul>
</li>
<li><a href="#_lab_09_testing_monitoring_and_troubleshooting">Lab 09 Testing, Monitoring and Troubleshooting</a>
<ul class="sectlevel2">
<li><a href="#_a_building_unit_tests">a. Building Unit Tests</a></li>
<li><a href="#_b_integration_tests_using_embedded_kafka">b. Integration Tests using Embedded Kafka</a></li>
<li><a href="#_c_getting_metrics_from_a_kafka_streams_application">c. Getting Metrics from a Kafka Streams Application</a></li>
<li><a href="#_d_using_jconsole_to_monitor_a_streams_app">d. Using JConsole to monitor a Streams App</a></li>
<li><a href="#_e_monitoring_a_kafka_streams_app_in_confluent_control_center">e. Monitoring a Kafka Streams App in Confluent Control Center</a></li>
</ul>
</li>
<li><a href="#_lab_11_security">Lab 11 Security</a>
<ul class="sectlevel2">
<li><a href="#_a_securing_a_kafka_streams_application">a. Securing a Kafka Streams Application</a></li>
</ul>
</li>
<li><a href="#_running_all_labs_with_docker">Appendix A: Running All Labs with Docker</a>
<ul class="sectlevel2">
<li><a href="#docker-local">Running Labs in Docker for Desktop</a></li>
<li><a href="#_running_the_exercise_applications">Running the Exercise Applications</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="content">
<div class="sect1">
<h2 id="_copyright_trademarks">Copyright &amp; Trademarks</h2>
<div class="sectionbody">
<div class="paragraph">
<p> <br>
 <br>
 <br>
 <br></p>
</div>
<div class="paragraph text-center">
<p><span class="big">Copyright © Confluent, Inc. 2014-2022. <a href="https://www.confluent.io/confluent-privacy-statement/">Privacy Policy</a> | <a href="https://www.confluent.io/terms-of-use/">Terms &amp; Conditions</a>.<br>
Apache, Apache Kafka, Kafka and the Kafka logo are trademarks of the<br>
<a href="http://www.apache.org/">Apache Software Foundation</a></span></p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_00_introduction">Lab 00 Introduction</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_a_introduction">a. Introduction</h3>
<div class="paragraph">
<p>This document provides Hands-On Exercises for the course <strong>Confluent Stream Processing using Apache Kafka® Streams &amp; ksqlDB</strong>. You will use a setup that includes a virtual machine (VM) configured with Apache Kafka and Confluent tools to manage your data and clusters.</p>
</div>
<div class="sect3">
<h4 id="_alternative_lab_environment">Alternative Lab Environment</h4>
<div class="paragraph">
<p>As an alternative you can:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Download the VM to your laptop and run it in VirtualBox. Make sure you have the newest version of VirtualBox installed. Download the VM from this link:</p>
<div class="paragraph">
<p><a href="https://s3.amazonaws.com/confluent-training-images-us-east-1/training-ubuntu-20-04-jan2022.ova" target="_blank" rel="noopener">https://s3.amazonaws.com/confluent-training-images-us-east-1/training-ubuntu-20-04-jan2022.ova</a></p>
</div>
</li>
<li>
<p>If you have installed Docker for Desktop on your Mac or Windows 10 Pro machine then you can run the labs there. But please note that your trainer might not be able to troubleshoot any potential problems if you are running the labs locally. If you choose to do this, follow the instructions at &#8594; <a href="#docker-local">Running Labs in Docker for Desktop</a>.</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_command_line_examples">Command Line Examples</h4>
<div class="paragraph">
<p>Most exercises contain commands that must be run from the command line. These commands will look like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>pwd</strong>
/home/training</pre>
</div>
</div>
<div class="paragraph">
<p>Commands you should type are shown in <strong>bold</strong>; non-bold text is an example of the output produced as a result of the command.</p>
</div>
</div>
<div class="sect3">
<h4 id="preparing-lab">Preparing the Labs</h4>
<div class="paragraph">
<p>Welcome to your lab environment! You are connected as user <strong>training</strong>, password <strong>training</strong>.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>If you haven&#8217;t already done so, you should open the <strong>Exercise Guide</strong> that is located on the lab virtual machine. To do so, open the <strong>Confluent Training Exercises</strong> folder that is located on the lab virtual machine desktop. Then double-click the shortcut that is in the folder to open the <strong>Exercise Guide</strong>.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/local-exercise-guide.png" alt="local exercise guide">
</div>
</div>
<div class="paragraph">
<p>Copy and paste works best if you copy from the Exercise Guide on your lab virtual machine.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Standard Ubuntu keyboard shortcuts will work: <code>Ctrl+C</code> &#8594; Copy, <code>Ctrl+V</code> &#8594; Paste</p>
</li>
<li>
<p>In a Terminal window: <code>Ctrl+Shift+C</code> &#8594; Copy, <code>Ctrl+Shift+V</code> &#8594; Paste.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>If you find these keyboard shortcuts are not working you can use the right-click context menu for copy and paste.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a terminal window</p>
</li>
<li>
<p>Clone the source code repository to the folder <code>confluent-streams</code> in your <strong>home</strong> directory:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~</strong>
$ <strong>git clone --depth 1 --branch 7.0.0-v1.0.1 \
    https://github.com/confluentinc/training-ksql-and-streams-src.git \
    confluent-streams</strong></pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
If you chose to select another folder for the labs then note that many of our samples assume that the lab folder is <code>~/confluent-streams</code>. You will have to adjust all those command to fit your specific environment.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Navigate to the <code>confluent-streams</code> folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams</strong></pre>
</div>
</div>
</li>
<li>
<p>Start the Kafka cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should see something similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Creating network "confluent-streams_kafka-net" with the default driver
Creating kafka          ... done
Creating zookeeper      ... done</pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>In the first steps of each exercise, you may launch the containers needed for the exercise with a <code>docker-compose up</code> command. Simply typing <code>docker-compose up -d</code> will start all of the containers defined in the docker-compose.yml file. You can start fewer containers by specifying only those you want to run, for example: <code>docker-compose up -d zookeeper kafka</code>.</p>
</div>
<div class="paragraph">
<p>The majority of the exercises use the docker-compose.yml file in the ~/confluent-streams directory. The <code>docker-compose up</code> command will search up the directory hierarchy until it finds a docker-compose.yml file, so the one in the confluent-streams directory will usually be used. The exception is the docker-compose.yml file used in the security exercise as this has additional security settings. See the comments at the beginning of Lab 11 Securing a Kafka Streams Application.</p>
</div>
<div class="paragraph">
<p>If at any time you want to get your environment back to a clean state use <code>docker-compose down</code> to end all of your containers. Then return to your last <code>docker-compose up</code> to get back to the beginning of an exercise.</p>
</div>
<div class="paragraph">
<p>Exercises do not need to be completed in order.  You can start from the beginning of any exercise at any time.</p>
</div>
<div class="paragraph">
<p>If you want to completely clear out your docker environment use the script on the VM: <code>docker-nuke.sh</code>. The nuke script will forcefully end all of your running docker containers.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Monitor the cluster with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose ps</strong>

   Name               Command            State              Ports
----------------------------------------------------------------------------
kafka       /etc/confluent/docker/run   Up      0.0.0.0:9092-&gt;9092/tcp
zookeeper   /etc/confluent/docker/run   Up      2181/tcp, 2888/tcp, 3888/tcp</pre>
</div>
</div>
<div class="paragraph">
<p>All services should have <code>State</code> equal to <code>Up</code>.</p>
</div>
</li>
<li>
<p>You can also observe the stats of Docker on your VM:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker stats</strong>

CONTAINER ID     NAME        CPU %    MEM USAGE / LIMIT   MEM %  NET I/O        BLOCK I/O     PIDS
ab9c97077e94     zookeeper   0.14%    88.14MiB / 9.737GiB 0.88%  106kB / 130kB    0B / 0B          48
ff47bece9e4f     kafka       1.17%    421.8MiB / 9.737GiB 4.23%  646kB / 522kB    0B / 0B          77</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Press <code>Ctrl+C</code> to exit the Docker statistics.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_b_continued_learning_after_class">b. Continued Learning After Class</h3>
<div class="paragraph">
<p>Once the course ends, the VM in Content Raven will terminate and you will no longer have access to it. However, you can still download the VM onto your own machine or use Docker locally to revisit these materials. We encourage you to bring up your own test environment, explore configuration files, inspect scripts, and perform tests. Here are some activities we encourage to reinforce your learning:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Revisit the exercises in this manual</p>
</li>
<li>
<p>Summarize and discuss the student handbook with your peers</p>
</li>
<li>
<p>Consult the README in this public repository for more resources and your own development playground: <a href="https://github.com/confluentinc/training-ksql-and-streams-src">https://github.com/confluentinc/training-ksql-and-streams-src</a></p>
</li>
</ul>
</div>
<div class="sect3">
<h4 id="_conclusion">Conclusion</h4>
<div class="paragraph">
<p>In this lab you have prepared and tested the Lab Environment. Finally you have created your Apache Kafka cluster that will be used in subsequent exercises.</p>
</div>
<div class="paragraph">
<p> <br>
 <br>
 <br></p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_01_introduction_to_kafka_streams">Lab 01 Introduction to Kafka Streams</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_a_scaling_a_kafka_streams_application">a. Scaling a Kafka Streams Application</h3>
<div class="paragraph">
<p>In this exercise, we&#8217;re going to write a Kafka producer in either Python or Java that generates a stream of temperature readings for a set of weather stations. We are also writing a simple Kafka Streams application that will consume this topic and calculate the maximum temperature per station per time window. We will then run this application in a single instance and later scale it up to several instances. We will monitor the throughput with the <strong>Confluent Control Center</strong>.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites">Prerequisites</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_kafka_cluster_and_confluent_control_center">Running the Kafka Cluster and Confluent Control Center</h4>
<div class="paragraph">
<p>To be able to run Kafka Streams applications we need a working Kafka cluster. We will run one consisting of a single broker and zookeeper.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Locate the file <code>docker-compose.yml</code> in the <code>~/confluent-streams</code> directory.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This <code>docker-compose.yml</code> file will be used to run a simple Kafka cluster as a backend for our Kafka Streams applications. If you&#8217;re curious, please open the file and analyze its contents.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Navigate to the folder <code>labs/scaling</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/scaling</strong></pre>
</div>
</div>
</li>
<li>
<p>Run the Kafka cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka ksqldb-server control-center</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Wait a couple of minutes until the cluster is initialized.</p>
</div>
</li>
<li>
<p>Create the two topics <code>temperature-readings</code> and <code>max-temperatures</code>, each with 3 partitions using these commands:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 3 \
    --topic temperature-readings</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 3 \
    --topic max-temperatures</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_creating_the_producer">Creating the Producer</h4>
<div class="paragraph">
<p>Now it is time to create a temperature readings producer. You can do it either in Python or Java. We start with the Python producer. If you prefer Java then move ahead to &#8594;<a href="#java-producer">here</a>.</p>
</div>
<div class="sect4">
<h5 id="_create_the_producer_in_python">Create the Producer in Python</h5>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to the folder <code>labs/scaling</code>, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/scaling/temp-producer</strong>
$ <strong>code .</strong></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
If a pop-up appears when VS Code opens, check the box and click <code>Yes, I trust the authors</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Locate the file <code>main.py</code>.</p>
</li>
<li>
<p>Inspect the code.</p>
<div class="ulist">
<ul>
<li>
<p>We are defining a few (temperature measurement) stations, their respective average temperatures and last measured temperatures</p>
</li>
<li>
<p>we&#8217;re using the Confluent Python client for Kafka to create a producer</p>
</li>
<li>
<p>every ~100 ms we&#8217;re generating a temperature reading for one of the randomly selected station.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Return to the terminal window, and install the python prerequisite.  Note this may already be installed from a previous exercise:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>pip3 install --upgrade pip</strong>
$ <strong>pip3 install confluent-kafka</strong></pre>
</div>
</div>
</li>
<li>
<p>Run the producer:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>python3 main.py</strong></pre>
</div>
</div>
</li>
<li>
<p>In another terminal window run the <code>kafka-console-consumer</code> to display the <code>temperature-readings</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --max-messages 25 \
    --topic temperature-readings \
    --property print.key=true \
    --property key.separator=", "</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should see an output similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>S-06, {"station": "S-06", "temperature": -1}
S-03, {"station": "S-03", "temperature": 8}
S-03, {"station": "S-03", "temperature": 9}
S-06, {"station": "S-06", "temperature": 0}
S-08, {"station": "S-08", "temperature": 31}
S-09, {"station": "S-09", "temperature": -7}
...</pre>
</div>
</div>
<div class="paragraph">
<p>The <code>key</code> is the station and the <code>value</code> is a JSON object with the station and the temperature in degree Celsius.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect4">
<h5 id="java-producer">Create the Producer in Java</h5>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to the folder <code>labs/scaling</code>, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/scaling/temp-producer</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Locate the file <code>build.gradle</code> and analyze its content. It is the build file for a simple Kafka client.</p>
</li>
<li>
<p>Locate the file <code>TempProducer.java</code> in the subfolder <code>src/main/java/streams</code> and open it. Analyze the file and make sure you understand the code. If necessary discuss with your peers.</p>
</li>
<li>
<p>Notice the file <code>log4j.properties</code> in the folder <code>src/main/resources</code> that configures logging for the producer.</p>
</li>
<li>
<p>Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> in VS Code or <code>./gradlew run</code> in the terminal to run the Java producer.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The first time you run the debugger it may take extra time while resources are downloaded.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>You should see an output similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash">The record is: S-06, <span class="o">{</span><span class="s2">"station"</span>: <span class="s2">"S-06"</span>, <span class="s2">"temperature"</span>: <span class="nt">-1</span><span class="o">}</span>
The record is: S-03, <span class="o">{</span><span class="s2">"station"</span>: <span class="s2">"S-03"</span>, <span class="s2">"temperature"</span>: 8<span class="o">}</span>
The record is: S-03, <span class="o">{</span><span class="s2">"station"</span>: <span class="s2">"S-03"</span>, <span class="s2">"temperature"</span>: 9<span class="o">}</span>
The record is: S-06, <span class="o">{</span><span class="s2">"station"</span>: <span class="s2">"S-06"</span>, <span class="s2">"temperature"</span>: 0<span class="o">}</span>
The record is: S-08, <span class="o">{</span><span class="s2">"station"</span>: <span class="s2">"S-08"</span>, <span class="s2">"temperature"</span>: 31<span class="o">}</span>
The record is: S-09, <span class="o">{</span><span class="s2">"station"</span>: <span class="s2">"S-09"</span>, <span class="s2">"temperature"</span>: <span class="nt">-7</span><span class="o">}</span>
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>key</code> is the station and the <code>value</code> is a JSON object with the station and temperature in degree Celsius.</p>
</div>
</li>
<li>
<p>From another terminal window, run the <code>kafka-console-consumer</code> to display the <code>temperature-readings</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --max-messages 25 \
    --topic temperature-readings \
    --property print.key=true \
    --property key.separator=", "</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should see an output similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>S-06, {"station": "S-06", "temperature": -1}
S-03, {"station": "S-03", "temperature": 8}
S-03, {"station": "S-03", "temperature": 9}
S-06, {"station": "S-06", "temperature": 0}
S-08, {"station": "S-08", "temperature": 31}
S-09, {"station": "S-09", "temperature": -7}
...</pre>
</div>
</div>
</li>
<li>
<p>Jump to the next section "Writing the Kafka Streams Application".</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_writing_the_kafka_streams_application">Writing the Kafka Streams Application</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a terminal window and navigate to the <code>streams-app</code> folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/scaling/streams-app</strong></pre>
</div>
</div>
</li>
<li>
<p>In the folder <code>confluent-streams/labs/scaling/streams-app</code> locate the file <code>build.gradle</code> and analyze its content.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In addition to the usual libraries we also load the <code>monitoring-interceptors</code> library to be able to integrate with the Confluent Control Center and the <code>kafka-json-serializer</code> library for the JSON serde.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>We will be using the Kafka Streams application called <code>StreamsApp.java</code> in the subfolder <code>src/main/java/streams</code> for this exercise.</p>
</li>
<li>
<p>You may choose to launch VS Code with <code>code .</code> to build and run the application.  Or simply use gradle with <code>./gradlew run</code>.</p>
</li>
<li>
<p>Run an instance of <code>kafka-console-consumer</code> to display the <code>max-temperatures</code> topic.  Note it may take some time for max temperatures to appear:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --topic max-temperatures</strong></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Is the output surprising to you? Why?
It is because of the nature of the <code>commit.interval.ms</code> property and how it relates to the output of KTables to Kafka topics. This will be discussed later in the course.</p>
</div>
<div class="paragraph">
<p>Ignore the warnings:<br>
<code>2022-05-11 21:45:52 WARN  ConsumerConfig:362 - The configuration 'admin.retry.backoff.ms' was supplied but isn&#8217;t a known config.
2022-05-11 21:45:52 WARN  ConsumerConfig:362 - The configuration 'admin.retries' was supplied but isn&#8217;t a known config.</code></p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Open Confluent Control Center at <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a></p>
</li>
<li>
<p>In Control Center, click on <code>CONTROLCENTER.CLUSTER</code> and then under <strong>Consumers</strong>, monitor the consumer lag for the <code>streams-app-v.0.1.0</code>. Note that the consumer group falls more and more behind:</p>
<div class="imageblock">
<div class="content">
<img src="./images/intro-streams/ksqlDB-consumer-lag.png" alt="ksqlDB consumer lag">
</div>
</div>
</li>
<li>
<p>Alternatively, you can use the Kafka tool <code>kafka-consumer-groups</code> to check the consumer lag:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-consumer-groups
    --bootstrap-server kafka:9092
    --group streams-app-v0.1.0
    --describe</strong>
GROUP              TOPIC                PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG    CONSUMER-ID...
streams-app-v0.1.0 temperature-readings 0          551             2417            1866   streams-app...
streams-app-v0.1.0 temperature-readings 1          363             5395            5032   streams-app...
streams-app-v0.1.0 temperature-readings 2          580             3614            3034   streams-app...</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_scaling_the_kafka_streams_application">Scaling the Kafka Streams Application</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open another terminal window to run another instance of your Kafka Streams application:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/scaling/streams-app</strong>
$ <strong>./gradlew run</strong></pre>
</div>
</div>
</li>
<li>
<p>In <strong>Confluent Control Center</strong> observe how the throughput of the streams app nearly doubles.</p>
</li>
<li>
<p>Note that we have now two consumer instances listed (recognizable by their ID):</p>
<div class="imageblock">
<div class="content">
<img src="./images/intro-streams/ksqlDB-consumer-lag2.png" alt="ksqlDB consumer lag2">
</div>
</div>
</li>
<li>
<p>Also observe that the consumer lag increases more slowly&#8230;&#8203;</p>
</li>
<li>
<p>Now scale the streams app to 3 instances and again monitor an increase in throughput and reduction in consumer lag.</p>
</li>
<li>
<p>Finally scale the app again, this time to 4 instances. Monitor the throughput after scaling the app. What are you observing? Explain your observation. See the conclusion for an explanation of what happens here.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_optional_using_the_ksqldb_cli">Optional: Using the ksqlDB CLI</h4>
<div class="paragraph">
<p>We can achieve the same results using ksqlDB!</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use the ksqlDB CLI to play with the data:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>ksql http://ksqldb-server:8088</strong></pre>
</div>
</div>
</li>
<li>
<p>Set the starting point of your queries to <code>earliest</code>:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SET 'auto.offset.reset' = 'earliest';</strong></pre>
</div>
</div>
</li>
<li>
<p>Create a stream for the source topic:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>CREATE STREAM temperatures(station STRING, temperature INTEGER)
        WITH(KAFKA_TOPIC='temperature-readings', VALUE_FORMAT='JSON');</strong></pre>
</div>
</div>
</li>
<li>
<p>Create a table that shows the maximum, minimum, and average temperatures per station per minute:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>CREATE TABLE temp_agg_per_min AS
        SELECT station,
                max(temperature) AS max,
                min(temperature) AS min,
                sum(temperature) / count( * ) AS avg
        FROM temperatures
        WINDOW TUMBLING (SIZE 1 MINUTE)
        GROUP BY station;</strong></pre>
</div>
</div>
</li>
<li>
<p>Inspect the aggregated temperature data as new records flow in from the producer.</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT station, max, min, avg FROM temp_agg_per_min EMIT CHANGES;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Press <code>CTL-C</code> to terminate the query.</p>
</div>
</li>
<li>
<p>Exit ksqlDB with <code>Ctrl+D</code>.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_2">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Stop the producer, consumers, and stream application with <code>Ctrl+C</code> in the terminal or <strong>Run</strong> &#8594; <strong>Stop Debugging</strong> in VScode.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_2">Conclusion</h4>
<div class="paragraph">
<p>In this exercise we have created a Kafka Streams application that processed an input topic and produced an output topic. First we ran only one application instance and then we scaled the application up to several instances. We noticed a significant boost in throughput until the number of instances was greater than the number of partitions of the input topic. At this point the additional application instances were sitting there idle.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
In the solutions folder, the Java producer and Kafka Streams app include Dockerfiles so that they can be deployed as containers. A separate <code>docker-compose.services.yml</code> has also been provided to start the microservices.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p> 
 
 </p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_02_working_with_kafka_streams">Lab 02 Working with Kafka Streams</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This lab contains 2 exercises:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Anatomy of a Kafka Streams App</p>
</li>
<li>
<p>Working with JSON</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_a_anatomy_of_a_kafka_streams_app">a. Anatomy of a Kafka Streams App</h3>
<div class="paragraph">
<p>In this exercise, you will create a Kafka Streams application and deploy it using Gradle (or, optionally, Maven). The purpose of this exercise is to illustrate the structure of Kafka Streams application code and the routine of deploying code with build tools.</p>
</div>
<div class="paragraph">
<p>The application itself reads data from a topic whose keys are integers and whose values are sentence strings. The application transforms the input so that the strings are lower-case and output to a new topic.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_2">Prerequisites</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_preparing_the_kafka_cluster">Preparing the Kafka Cluster</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to the folder <code>~/confluent-streams/labs/working-streams</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/working-streams</strong></pre>
</div>
</div>
</li>
<li>
<p>From within the <code>working-streams</code> folder run the cluster with the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka</strong>
Creating network "confluent-streams_kafka-net" with the default driver
Creating kafka           ... done
Creating zookeeper       ... done</pre>
</div>
</div>
</li>
<li>
<p>Double check that the cluster is up and running:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose ps</strong></pre>
</div>
</div>
<div class="paragraph">
<p>you should see something similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>         Name                     Command            State              Ports
-----------------------------------------------------------------------------------------
kafka                       /etc/confluent/docker/run   Up      0.0.0.0:9092-&gt;9092/tcp
zookeeper                   /etc/confluent/docker/run   Up      2181/tcp, 2888/tcp, 3888/tcp</pre>
</div>
</div>
<div class="paragraph">
<p>Make sure all services have <code>State=Up</code>.</p>
</div>
</li>
<li>
<p>Create an input topic called <code>lines-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic lines-topic</strong></pre>
</div>
</div>
</li>
<li>
<p>Create an output topic called <code>lines-lower-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic lines-lower-topic</strong>
Created topic "lines-lower-topic".</pre>
</div>
</div>
</li>
<li>
<p>Let&#8217;s now check what topics are in Kafka using this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --bootstrap-server kafka:9092 \
    --list</strong></pre>
</div>
</div>
<div class="paragraph">
<p>We should see something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>__confluent.support.metrics
lines-lower-topic
lines-topic</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
If you need to delete a topic, say the one with name <code>&lt;topic name&gt;</code>, (e.g. to start over) you can use this command:
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --bootstrap-server kafka:9092 \
    --delete \
    --topic &lt;topic name&gt;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Now we are ready to create, build and run our first <strong>Kafka Streams</strong> application. We will first build and run the application using Java in conjunction with <strong>Gradle</strong>. Optionally, we will also see how to build and run the application with <strong>Maven</strong>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_authoring_the_kafka_streams_application_using_java_gradle">Authoring the Kafka Streams Application using Java &amp; Gradle</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to the folder <code>gradle-sample</code> with in the <code>working-streams</code> folder, and launch VS Code:</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Be certain to include the period in the <code>code .</code> command below. That indicates starting VS Code in the current directory - otherwise some references may not correctly resolve.
</td>
</tr>
</table>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/working-streams/gradle-sample</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>In this folder locate the <code>build.gradle</code> file and open it to analyze its content.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The external dependencies for a simple <strong>Kafka Streams</strong> app are the <code>kafka-clients</code> and <code>kafka-streams</code> libraries. We also add the <code>slf4j-log4j12</code> dependency for logging purposes.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>In VS Code, open the file <code>MapSample.java</code> in the subfolder <code>src/main/java/streams</code> and inspect its content. From here, you can challenge yourself to complete the TODOs, or you can move forward to see a step-by-step walkthrough of the code. If you decide to challenge yourself, you can always peek at the corresponding subfolder in <code>~/confluent-streams/solutions/</code> if you get stuck.</p>
</li>
<li>
<p>Now we start to add actual <strong>Kafka Streams</strong> application logic. We will start with the configuration part. Please add this code snippet to the <code>main</code> method of the class (right after the initial <code>println</code>):</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nc">Properties</span> <span class="n">settings</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">settings</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">StreamsConfig</span><span class="o">.</span><span class="na">APPLICATION_ID_CONFIG</span><span class="o">,</span> <span class="s">"map-sample-v0.1.0"</span><span class="o">);</span>
<span class="n">settings</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">StreamsConfig</span><span class="o">.</span><span class="na">BOOTSTRAP_SERVERS_CONFIG</span><span class="o">,</span> <span class="s">"kafka:9092"</span><span class="o">);</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>We&#8217;re providing an ID to our application and tell it where to find the Kafka cluster. This is the minimal configuration needed!</p>
</div>
</li>
<li>
<p>Next we will define the topology of our <strong>Kafka Streams</strong> application. Add this snippet right after the configuration part:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="kd">final</span> <span class="nc">Serde</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">&gt;</span> <span class="n">stringSerde</span> <span class="o">=</span> <span class="nc">Serdes</span><span class="o">.</span><span class="na">String</span><span class="o">();</span>
<span class="nc">StreamsBuilder</span> <span class="n">builder</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StreamsBuilder</span><span class="o">();</span>
<span class="nc">KStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">String</span><span class="o">&gt;</span> <span class="n">lines</span> <span class="o">=</span> <span class="n">builder</span>
    <span class="o">.</span><span class="na">stream</span><span class="o">(</span><span class="s">"lines-topic"</span><span class="o">,</span> <span class="nc">Consumed</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">stringSerde</span><span class="o">));</span>
<span class="nc">KStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">String</span><span class="o">&gt;</span> <span class="n">transformed</span> <span class="o">=</span> <span class="n">lines</span>
    <span class="o">.</span><span class="na">mapValues</span><span class="o">(</span><span class="n">value</span> <span class="o">-&gt;</span> <span class="n">value</span><span class="o">.</span><span class="na">toLowerCase</span><span class="o">());</span>
<span class="n">transformed</span><span class="o">.</span><span class="na">to</span><span class="o">(</span><span class="s">"lines-lower-topic"</span><span class="o">,</span> <span class="nc">Produced</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">stringSerde</span><span class="o">));</span>
<span class="nc">Topology</span> <span class="n">topology</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="na">build</span><span class="o">();</span></code></pre>
</div>
</div>
<div class="paragraph">
<p> 
<br>
We&#8217;re defining a builder for the topology, use it to create a <code>KStream</code> from the Kafka topic <code>line-topic</code> using <code>String</code> Serdes for both key and value of the messages. Then we&#8217;re using a <code>mapValues</code> function on the <code>KStream</code> to convert the <code>value</code> into all lower case. Finally we&#8217;re writing the result into the Kafka topic <code>line-lower-topic</code>. Then we build the topology.</p>
</div>
</li>
<li>
<p>With the settings and the topology at hand we can now create the <strong>Streams</strong> app:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nc">KafkaStreams</span> <span class="n">streams</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">KafkaStreams</span><span class="o">(</span><span class="n">topology</span><span class="o">,</span> <span class="n">settings</span><span class="o">);</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Our application will now start consuming data from the input topic, transform it and write it to the output topic.</p>
</div>
</li>
<li>
<p>To have our application terminate in an orderly way when requested without leaving any resource leaks behind we add a <strong>shutdown hook</strong> at the end of the main method:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="kd">final</span> <span class="nc">CountDownLatch</span> <span class="n">latch</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">CountDownLatch</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
<span class="nc">Runtime</span><span class="o">.</span><span class="na">getRuntime</span><span class="o">().</span><span class="na">addShutdownHook</span><span class="o">(</span><span class="k">new</span> <span class="nc">Thread</span><span class="o">(()</span> <span class="o">-&gt;</span> <span class="o">{</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(</span><span class="s">"### Stopping Map Sample Application ###"</span><span class="o">);</span>
    <span class="n">streams</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
    <span class="n">latch</span><span class="o">.</span><span class="na">countDown</span><span class="o">();</span>
 <span class="o">}));</span>

<span class="k">try</span><span class="o">{</span>
    <span class="n">streams</span><span class="o">.</span><span class="na">start</span><span class="o">();</span>
    <span class="n">latch</span><span class="o">.</span><span class="na">await</span><span class="o">();</span>
<span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="kd">final</span> <span class="nc">Throwable</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
    <span class="nc">System</span><span class="o">.</span><span class="na">exit</span><span class="o">(</span><span class="mi">1</span><span class="o">);</span>
<span class="o">}</span>
<span class="nc">System</span><span class="o">.</span><span class="na">exit</span><span class="o">(</span><span class="mi">0</span><span class="o">);</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>This <strong>Shutdown Hook</strong> will be executed when the application receives a <code>SIG_TERM</code> signal. The <code>CountDownLatch</code> is used as a best practice to avoid rare cases of deadlock. Notice the use of the <code>await()</code> method after the application starts.</p>
</div>
</li>
<li>
<p>Within the project folder <code>gradle-sample</code> locate the folder for the Java resources <code>src/main/resources</code>. In this folder we have a file <code>log4j.properties</code>. This file is used to configure the logger for our Kafka Streams app.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Use <code>INFO</code> instead of <code>WARN</code> for the <code>rootLogger</code> if you want to be more verbose in the logs.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>And that&#8217;s all we need. Our first <strong>Kafka Streams</strong> app is ready to go!  Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> to run your code.</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/streams-working/debugger.png" alt="debugger">
</div>
</div>
<div class="paragraph">
<p>In the VS Code DEBUG CONSOLE tab you should see something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="k">***</span> Starting Map Sample Application <span class="k">***</span>
2018-07-04 13:47:36 WARN  ConsumerConfig:287 - The configuration <span class="s1">'admin.retries'</span> was supplied but isn<span class="s1">'t a known config.</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>At this time you can safely ignore the WARN log items.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
At this time nothing will happen since we did not yet produce any data in the <code>lines-topic</code> in Kafka.  Let your Kafka Streams app remain running in the VS Code debugger.  This will be our next task <strong>after</strong> we have shown how to build the same app using Maven instead of Gradle.<br>
If you want to skip the <strong>Maven</strong> part then go to <a href="#producing-input-data">Producing some Input Data</a>.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_optional_build_and_run_the_application_with_java_maven">Optional: Build and Run the application with Java &amp; Maven</h4>
<div class="paragraph">
<p>Here we&#8217;re basically showing the same steps as in the previous section, except we will now use <strong>Maven</strong> instead of <strong>Gradle</strong>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to the <code>maven-sample</code> folder, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/working-streams/maven-sample</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Locate the <code>pom.xml</code> file, open it and analyze its content.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
this is the minimum Maven file required to compile, package and run a Java application that has external dependencies on the 2 required libraries <code>kafka-clients</code> and <code>kafka-streams</code>. It also uses the <code>slf4j-log4j12</code> library for logging.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Locate the file <code>MapSample.java</code> in the subfolder <code>src/main/java/streams</code> and open it. Double check that it looks similar to the one you created in the Gradle example.</p>
</li>
<li>
<p>Note the same file <code>log4j.properties</code> in the folder <code>src/main/resources</code> as in the Gradle example.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Use <code>INFO</code> instead of <code>WARN</code> for the <code>rootLogger</code> if you want to be more verbose in the logs.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Solution code has been provided in this exercise, so our first <strong>Kafka Streams</strong> app is ready to go! Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> to run your code.</p>
<div class="paragraph">
<p>You should see something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="k">***</span> Starting Map Sample Application <span class="k">***</span>
2018-07-04 13:53:12 WARN  ConsumerConfig:287 - The configuration <span class="s1">'admin.retries'</span> was supplied but isn<span class="s1">'t a known config.</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>At this time you can safely ignore the WARN log items.</p>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
At this time nothing will happen since we did not yet produce any data in the <code>lines-topic</code> in Kafka. Let&#8217;s do this next.  Let your Kafka Streams app remain running in the VS Code debugger.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="producing-input-data">Producing some Input Data</h4>
<div class="paragraph">
<p>We&#8217;re going to use the <code>kafka-console-producer</code> tool to create some input data in the topic <code>lines-topic</code>.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a new terminal window and navigate to the <code>working-streams</code> folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/working-streams</strong></pre>
</div>
</div>
</li>
<li>
<p>From a terminal window execute this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cat &lt;&lt; EOF | kafka-console-producer \
    --bootstrap-server kafka:9092 \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --topic lines-topic
1:"Kafka powers the Confluent Streaming Platform"
2:"Events are stored in Kafka"
3:"Confluent contributes to Kafka"
EOF</strong></pre>
</div>
</div>
<div class="paragraph">
<p>This writes 3 entries into the topic called <code>lines-topic</code> using <code>String</code> serializers for both key and value.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_reading_the_transformed_messages">Reading the Transformed Messages</h4>
<div class="paragraph">
<p>Here we&#8217;re using the <code>kafka-console-consumer</code> tool to read from the output topic. We&#8217;re again using String deserializers for key and value.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In the same terminal window, run:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --topic lines-lower-topic</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should see this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="s2">"kafka powers the confluent streaming platform"</span>
<span class="s2">"events are stored in kafka"</span>
<span class="s2">"confluent contributes to kafka"</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
You might need to be a bit patient until the messages appear. It can take a few seconds to a minute or so depending on the performance of your computer&#8230;&#8203;
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_3">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Terminate the Kafka console consumer by pressing <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Terminate your <strong>Kafka Streams</strong> application with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>. If you used the <code>./gradlew run</code> command instead, you can terminate with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_3">Conclusion</h4>
<div class="paragraph">
<p>We have created our first complete <strong>Kafka Streams</strong> application. We built the application using Gradle, and then again with Maven. We have used the command line tools provided by Kafka to produce input data and display the transformed output data.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div class="sect2">
<h3 id="_b_working_with_json">b. Working with JSON</h3>
<div class="paragraph">
<p>The purpose of this exercise is to learn how to create serializers and deserializers for custom Java objects.</p>
</div>
<div class="paragraph">
<p>In this case, we will create a Serde for an object that records temperature data using the <code>KafkaJsonSerializer</code> and <code>KafkaJsonDeserializer</code> helper classes. The application itself reads temperature data from an input topic, filters for temperatures higher than 25 degrees, and outputs that data to a new output topic.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_3">Prerequisites</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to this lab&#8217;s folder <code>~/confluent-streams/labs/working-streams</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/working-streams</strong></pre>
</div>
</div>
</li>
<li>
<p>If it is not already running, start the Kafka cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Do not proceed until all services are up and running; test with:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose ps</strong></pre>
</div>
</div>
<div class="paragraph">
<p>and assert that all services are in state <code>Up</code>.</p>
</div>
</li>
<li>
<p>Create an input topic called <code>temperatures-topic</code> and an output topic called <code>high-temperatures-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic temperatures-topic</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic high-temperatures-topic</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_writing_the_kafka_streams_app">Writing the Kafka Streams App</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a new terminal window and navigate to the <code>working-streams/json-sample</code> folder, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/working-streams/json-sample</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Locate the file <code>build.gradle</code> in this folder and open it to analyze its content.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Compared to the <code>build.gradle</code> file in the previous exercise we have added the <code>kafka-json-serializer</code> library for the JSON serializer/deserializer
</td>
</tr>
</table>
</div>
</li>
<li>
<p>In the subfolder <code>src/main/java/streams</code> locate the file <code>JsonSample.java</code> and familiarize yourself with the code. It basically does the configuration of the <strong>Kafka Streams</strong> app but the interesting code is missing. As before, you can challenge yourself to implement the missing code or follow the step-by-step instructions in this book.</p>
</li>
<li>
<p>To create a <strong>Serde</strong> (Serializer/Deserializer) for JSON formatted data, add this code to the function <code>getJsonSerde()</code> after the <strong>TODO</strong> comment:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nc">Map</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Object</span><span class="o">&gt;</span> <span class="n">serdeProps</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">HashMap</span><span class="o">&lt;&gt;();</span>
<span class="n">serdeProps</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="s">"json.value.type"</span><span class="o">,</span> <span class="nc">TempReading</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

<span class="kd">final</span> <span class="nc">Serializer</span><span class="o">&lt;</span><span class="nc">TempReading</span><span class="o">&gt;</span> <span class="n">temperatureSerializer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">KafkaJsonSerializer</span><span class="o">&lt;&gt;();</span>
<span class="n">temperatureSerializer</span><span class="o">.</span><span class="na">configure</span><span class="o">(</span><span class="n">serdeProps</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>

<span class="kd">final</span> <span class="nc">Deserializer</span><span class="o">&lt;</span><span class="nc">TempReading</span><span class="o">&gt;</span> <span class="n">temperatureDeserializer</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">KafkaJsonDeserializer</span><span class="o">&lt;&gt;();</span>
<span class="n">temperatureDeserializer</span><span class="o">.</span><span class="na">configure</span><span class="o">(</span><span class="n">serdeProps</span><span class="o">,</span> <span class="kc">false</span><span class="o">);</span>

<span class="k">return</span> <span class="nc">Serdes</span><span class="o">.</span><span class="na">serdeFrom</span><span class="o">(</span><span class="n">temperatureSerializer</span><span class="o">,</span> <span class="n">temperatureDeserializer</span><span class="o">);</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>We&#8217;re basically using the two helper classes <code>KafkaJsonSerializer</code> and <code>KafkaJsonDeserializer</code> to create a serializer and a deserializer which in turn we then use to create a <strong>Serde</strong>.<br>
We will use this Serde to serialize and deserialize our <code>TempReading</code> POJO.</p>
</div>
</li>
<li>
<p>The final thing to do is to define the <strong>Topology</strong> for our application. We want to keep it simple and just filter the input topic <code>temperatures-topic</code> for high temperatures (&gt;25 degrees) and output the result to the output topic <code>high-temperatures-topic</code>. Add this code to the <code>getTopology()</code> function after the <strong>TODO</strong> comment:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="n">builder</span><span class="o">.</span><span class="na">stream</span><span class="o">(</span><span class="s">"temperatures-topic"</span><span class="o">,</span> <span class="nc">Consumed</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">temperatureSerde</span><span class="o">))</span>
    <span class="o">.</span><span class="na">filter</span><span class="o">((</span><span class="n">key</span><span class="o">,</span><span class="n">value</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="n">value</span><span class="o">.</span><span class="na">temperature</span> <span class="o">&gt;</span> <span class="mi">25</span><span class="o">)</span>
    <span class="o">.</span><span class="na">to</span><span class="o">(</span><span class="s">"high-temperatures-topic"</span><span class="o">,</span> <span class="nc">Produced</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">temperatureSerde</span><span class="o">));</span>
<span class="k">return</span> <span class="n">builder</span><span class="o">.</span><span class="na">build</span><span class="o">();</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Note how the filter function uses the <code>value</code> which is an object of type <code>TempReading</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Note the file <code>log4j.properties</code> in the folder <code>src/main/resources</code> which is used to configure logging for our application.</p>
</li>
<li>
<p>Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> to run your code.  Let your Kafka Streams app remain running in the VS Code debugger.</p>
<div class="paragraph">
<p>You should get this output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="k">***</span> Starting JSON Sample Application <span class="k">***</span>
...</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_creating_input_data">Creating input Data</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Switch back to the terminal.</p>
</li>
<li>
<p>Use the following command to generate some temperature readings in JSON format:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cat &lt;&lt; EOF | kafka-console-producer \
    --bootstrap-server kafka:9092 \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --topic temperatures-topic
"S1":{"station":"S1", "temperature": 10.2, "timestamp": 1}
"S1":{"station":"S1", "temperature": 11.2, "timestamp": 2}
"S1":{"station":"S1", "temperature": 11.1, "timestamp": 3}
"S1":{"station":"S1", "temperature": 12.5, "timestamp": 4}
"S2":{"station":"S2", "temperature": 15.2, "timestamp": 1}
"S2":{"station":"S2", "temperature": 21.7, "timestamp": 2}
"S2":{"station":"S2", "temperature": 25.1, "timestamp": 3}
"S2":{"station":"S2", "temperature": 27.8, "timestamp": 4}
EOF</strong></pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Run this command repeatedly to generate more messages&#8230;&#8203;
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_reading_the_output">Reading the Output</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use the following command to read the output generated:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --topic high-temperatures-topic</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should get this output showing only readings with temperature higher than 25 degrees:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="o">{</span><span class="s2">"station"</span>:<span class="s2">"S2"</span>,<span class="s2">"temperature"</span>:25.1,<span class="s2">"timestamp"</span>:3<span class="o">}</span>
<span class="o">{</span><span class="s2">"station"</span>:<span class="s2">"S2"</span>,<span class="s2">"temperature"</span>:27.8,<span class="s2">"timestamp"</span>:4<span class="o">}</span></code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_4">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Terminate the Kafka console consumer by pressing <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Terminate your <strong>Kafka Streams</strong> application with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_4">Conclusion</h4>
<div class="paragraph">
<p>In this sample we have built a <strong>Kafka Streams</strong> application that uses custom serializer and deserializer to work with data that is JSON formatted.</p>
</div>
<div class="paragraph">
<p> <br>
 <br></p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_03_introduction_to_ksqldb">Lab 03 Introduction to ksqlDB</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_a_introduction_to_ksqldb">a. Introduction to ksqlDB</h3>
<div class="paragraph">
<p>In this lab exercise, you will use the ksqlDB CLI to slice and dice data that is being generated in real time. The purpose is to get a sense for the streaming applications that are possible using only a SQL-like syntax. We will query running streams, apply filters and maps, and create new streams and tables derived from existing streams.</p>
</div>
<div class="paragraph">
<p>For more details on running ksqlDB in Docker containers, please see <a href="https://docs.ksqldb.io/en/latest/operate-and-deploy/installation/install-ksqldb-with-docker/">Install ksqlDB with Docker</a>.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_4">Prerequisites</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_preparing_the_platform">Preparing the Platform</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a terminal window and navigate to the folder <code>~/confluent-streams</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams</strong></pre>
</div>
</div>
</li>
<li>
<p>This exercise will use the docker-compose.yml file in ~/confluent-streams, but this time we need to also run the ksqldb-server container.</p>
<div class="paragraph">
<p>Optional: Open the file in your editor and analyze its content. You do not necessarily need to understand all of it at this point.</p>
</div>
</li>
<li>
<p>Run the application with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka ksqldb-server</strong></pre>
</div>
</div>
</li>
<li>
<p>Wait until the app is up and running, that is all services are marked as <code>up</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose ps</strong></pre>
</div>
</div>
<div class="paragraph">
<p>you should see something like this:</p>
</div>
</li>
</ol>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code>    Name                 Command            State              Ports
--------------------------------------------------------------------------------
kafka           /etc/confluent/docker/run   Up      0.0.0.0:9092-&gt;9092/tcp
ksqldb-server   /etc/confluent/docker/run   Up      0.0.0.0:8088-&gt;8088/tcp
zookeeper       /etc/confluent/docker/run   Up      2181/tcp, 2888/tcp, 3888/tcp</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>You can see that we have 3 containers running on our system:</p>
<div class="ulist">
<ul>
<li>
<p>the first in the list is running a <strong>Kafka broker</strong></p>
</li>
<li>
<p>the second container runs an instance of <strong>ksqlDB Server</strong></p>
</li>
<li>
<p>and the last one runs an instance of <strong>Zookeeper</strong></p>
<div class="paragraph">
<p>All these containers run on a software defined network (SDN) called <code>confluent-streams_kafka-net</code>.</p>
</div>
</li>
</ul>
</div>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_ksqldb_cli">Running the ksqlDB CLI</h4>
<div class="paragraph">
<p>We want now to use the ksqlDB CLI and connect it with our ksqlDB server.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="5">
<li>
<p>Start the ksqlDB CLI</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>ksql http://ksqldb-server:8088</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You will be greeted by the following screen:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release.

                  ===========================================
                  =       _              _ ____  ____       =
                  =      | | _____  __ _| |  _ \| __ )      =
                  =      | |/ / __|/ _` | | | | |  _ \      =
                  =      |   &lt;\__ \ (_| | | |_| | |_) |     =
                  =      |_|\_\___/\__, |_|____/|____/      =
                  =                   |_|                   =
                  =  Event Streaming Database purpose-built =
                  =        for stream processing apps       =
                  ===========================================

Copyright 2017-2021 Confluent Inc.

CLI v7.0.0, Server v7.0.0 located at http://localhost:8088
Server Status: RUNNING

Having trouble? Type 'help' (case-insensitive) for a rundown of how things work!

ksql&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_working_with_ksqldb_cli">Working with ksqlDB CLI</h4>
<div class="paragraph">
<p>Here we&#8217;re going to experiment with various features of ksqlDB. We use the two topics <code>pageviews</code> and <code>users</code>.</p>
</div>
<div class="paragraph">
<p>First we will use the ksql-datagen tool to create data for us. This tool has a number of predefined data types - you see us requesting them here with the parameter <code>quickstart=users</code> and <code>quickstart=pageviews</code> in the commands below.</p>
</div>
<div class="paragraph">
<p>For more info on this tool, see: <a href="https://docs.ksqldb.io/en/latest/developer-guide/test-and-debug/generate-custom-test-data/">https://docs.ksqldb.io/en/latest/developer-guide/test-and-debug/generate-custom-test-data/</a>.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="6">
<li>
<p>First, create the two topics <code>users</code> and <code>pageviews</code>. Run:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics --bootstrap-server kafka:9092 --create --partitions 1 --replication-factor 1 --topic users</strong>

$ <strong>kafka-topics --bootstrap-server kafka:9092 --create --partitions 1 --replication-factor 1 --topic pageviews</strong></pre>
</div>
</div>
</li>
<li>
<p>Open two more terminals and run each of the ksql-datagen commands in one of them.</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>ksql-datagen quickstart=users format=json topic=users \
     bootstrap-server=kafka:9092</strong>

$ <strong>ksql-datagen quickstart=pageviews format=delimited \
     topic=pageviews bootstrap-server=kafka:9092</strong></pre>
</div>
</div>
</li>
<li>
<p>Now, return to the ksqlDB CLI. To be able to work with data from Kafka we need to create either a <strong>Stream</strong> or a <strong>Table</strong> in ksqlDB. Let&#8217;s create a stream from the topic <code>pageviews</code> using KSQL:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>CREATE STREAM pageviews_original (
        viewtime bigint,
        userid varchar,
        pageid varchar
    ) WITH (kafka_topic='pageviews', value_format='DELIMITED');</strong></pre>
</div>
</div>
<div class="paragraph">
<p>This command creates a <strong>Stream</strong> called <code>pageviews_original</code> from the Kafka topic <code>pageviews</code>, whose record values are encoded in CSV (here called <code>DELIMITED</code>). The three values in each record value are interpreted as fields <code>viewtime</code>, <code>userid</code> and <code>pageid</code>.</p>
</div>
<div class="paragraph">
<p>The ksqlDB editor should answer with</p>
</div>
<div class="literalblock">
<div class="content">
<pre>Message
----------------
Stream created
----------------</pre>
</div>
</div>
</li>
<li>
<p>We can now describe the stream:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>DESCRIBE pageviews_original;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>giving us this output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Name                 : PAGEVIEWS_ORIGINAL
 Field    | Type
----------------------------
 VIEWTIME | BIGINT
 USERID   | VARCHAR(STRING)
 PAGEID   | VARCHAR(STRING)
----------------------------
For runtime statistics and query details run: DESCRIBE EXTENDED &lt;Stream,Table&gt;;</pre>
</div>
</div>
</li>
<li>
<p>Let&#8217;s create a <strong>table</strong> from the <code>users</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>CREATE TABLE users_original (
      registertime BIGINT,
      gender VARCHAR,
      regionid VARCHAR,
      userid VARCHAR PRIMARY KEY
    ) WITH
    (kafka_topic='users', value_format='JSON');</strong></pre>
</div>
</div>
<div class="paragraph">
<p>This command creates a <strong>Table</strong> from the Kafka topic <code>users</code>, whose records have values encoded in <code>JSON</code>. Since this is a table, we need a key by which the records from the source topic are grouped. In our case this is the field <code>userid</code>. The four fields <code>registertime</code>, <code>gender</code>, <code>regionid</code>, and <code>userid</code> should be part of the <code>JSON</code> value of the records in the topic.</p>
</div>
</li>
<li>
<p>Use <code>DESCRIBE users_original;</code> to get a description of the table.</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>DESCRIBE users_original;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>giving us this output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Name                 : USERS_ORIGINAL
 Field        | Type
-----------------------------------------------
 REGISTERTIME | BIGINT
 GENDER       | VARCHAR(STRING)
 REGIONID     | VARCHAR(STRING)
 USERID       | VARCHAR(STRING)  (primary key)
-----------------------------------------------
For runtime statistics and query details run: DESCRIBE EXTENDED &lt;Stream,Table&gt;;</pre>
</div>
</div>
</li>
<li>
<p>Now use <code>SHOW STREAMS;</code> and <code>SHOW TABLES;</code> to view the list of streams and tables defined in the system.</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SHOW STREAMS;</strong>
ksql&gt; <strong>SHOW TABLES;</strong></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
All ksqlDB commands need to be terminated with a semi-colon (;). ksqlDB SQL keywords such as CREATE or DESCRIBE are not case sensitive.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_querying_streams_and_tables">Querying Streams and Tables</h4>
<div class="olist arabic">
<ol class="arabic" start="13">
<li>
<p>Let&#8217;s get some data from the <code>pageviews_original</code> stream:</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
The default for where to start when selecting data is latest. If data is not being continuously loaded into your stream, this may result in your SELECT not displaying any data for some time. Set the default to be earliest in your ksqlDB CLI session with the set command: <strong>set 'auto.offset.reset'='earliest';</strong>
</td>
</tr>
</table>
</div>
<div class="literalblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM pageviews_original EMIT CHANGES LIMIT 10;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Please note the <code>LIMIT</code> clause which limits the output to 10 records. The output should look similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+--------------------+--------------------+--------------------+
|VIEWTIME            |USERID              |PAGEID              |
+--------------------+--------------------+--------------------+
|1603202197108       |User_5              |Page_22             |
|1603202197145       |User_6              |Page_55             |
|1603202197145       |User_1              |Page_82             |
|1603202197145       |User_4              |Page_86             |
|1603202197145       |User_2              |Page_58             |
|1603202197145       |User_8              |Page_97             |
|1603202197145       |User_6              |Page_88             |
|1603202197145       |User_4              |Page_21             |
|1603202197145       |User_5              |Page_65             |
|1603202197145       |User_5              |Page_34             |
Limit Reached
Query terminated</pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
If your data is displayed in columns that are too wide, you can change the column width in your ksqlDB CLI session using the set command. For example:
<strong>SET CLI COLUMN-WIDTH 15;</strong>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Let&#8217;s run the same query but this time without the <code>LIMIT</code> clause:</p>
<div class="literalblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM pageviews_original EMIT CHANGES;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>you will notice that the query does not stop and continues indefinitely. This is of course expected since a stream never ends.</p>
</div>
<div class="paragraph">
<p>Hit <code>Ctrl+C</code> to end the query.</p>
</div>
</li>
<li>
<p>Now try the same with the table:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM users_original EMIT CHANGES LIMIT 5;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>giving us this output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+--------------------+--------------------+--------------------+--------------------+
|USERID              |REGISTERTIME        |GENDER              |REGIONID            |
+--------------------+--------------------+--------------------+--------------------+
|User_2              |1512186975750       |FEMALE              |Region_4            |
|User_2              |1507817615345       |MALE                |Region_3            |
|User_2              |1493582927082       |FEMALE              |Region_3            |
|User_5              |1515794322305       |FEMALE              |Region_7            |
|User_2              |1514239675179       |FEMALE              |Region_9            |
Limit Reached
Query terminated</pre>
</div>
</div>
</li>
</ol>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Although tables are compacted topics and can be compared to classical database tables a query on them never ends analogous to the stream.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_filtering_and_mapping_operations">Filtering and Mapping Operations</h4>
<div class="paragraph">
<p>Similar to what we&#8217;re used to from SQL we can filter data from a stream (or from a table).</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="16">
<li>
<p>Let&#8217;s only display records from the stream whose user ID is equal to <code>User_1</code>:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM pageviews_original WHERE userid='User_1' EMIT CHANGES;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Hit <code>Ctrl+C</code> to end the query.</p>
</div>
</li>
<li>
<p>Now only records whose page is in the range 60 to 69:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM pageviews_original
      WHERE pageid LIKE 'Page_6%'
      EMIT CHANGES;</strong></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
The correct filter would be <code>Page_6_</code> where we want to only match one character after the 6, but ksqlDB currently only supports the wildcard <code>%</code> in the filter, matching zero or more characters. See <a href="https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/select-push-query/#like">https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/select-push-query/#like</a>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>We can selectively output information from the stream. Only display the field <code>userid</code> and <code>pageid</code>:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT pageid, userid
      FROM pageviews_original
      EMIT CHANGES
      LIMIT 5;</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleaning_up">Cleaning Up</h4>
<div class="olist arabic">
<ol class="arabic" start="19">
<li>
<p>Exit ksqlDB with <code>Ctrl+D</code>.</p>
</li>
<li>
<p>Return to the terminals running ksql-datagen and stop them using <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_5">Conclusion</h4>
<div class="paragraph">
<p>In this lab we have authored our very first ksqlDB queries and run them against two topics <code>pageviews</code> and <code>users</code> in Kafka. Kafka and all the other components of the ksqlDB platform ran in Docker containers to make the setup very easy and portable.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_b_using_the_ksqldb_rest_api">b. Using the ksqlDB REST API</h3>
<div class="paragraph">
<p>The idea of this exercise is to show how ksqlDB server can be accessed via its RESTful API by any language that can do <code>HTTP POST</code> requests. It is possible to create stream processing applications in any such language by submitting queries to a ksqlDB server cluster. The example in this exercise is a Python application. The Python application is also equipped with Kafka client libraries to produce input data and read output data, but in the real world, there are often dedicated upstream producer and downstream consumer applications, and so the stream processing application would not even need to import Kafka client libraries. ksqlDB enables essentially any language to create real-time stream processing applications via its REST API.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_5">Prerequisites</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Please make sure you have prepared your lab environment as described here: &#8594;<a href="#preparing-lab">Lab Environment</a></p>
</li>
<li>
<p>Navigate to the folder <code>labs/ksql-rest-api</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/ksql-rest-api</strong></pre>
</div>
</div>
</li>
<li>
<p>Run a Kafka cluster and a ksqlDB server using this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka ksqldb-server</strong>
Creating network "confluent-streams_kafka-net" with the default driver
Creating ksqldb-server     ... done
Creating zookeeper       ... done
Creating kafka           ... done</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Wait a couple of minutes until the cluster is ready.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_authoring_a_python_client">Authoring a Python Client</h4>
<div class="olist arabic">
<ol class="arabic" start="4">
<li>
<p>In the folder <code>labs/ksql-rest-api</code>, there is a file called <code>requirements.txt</code>. It has this content:</p>
<div class="literalblock">
<div class="content">
<pre>confluent-kafka==1.4.1
requests</pre>
</div>
</div>
<div class="paragraph">
<p>These are our external dependencies. The library <code>confluent-kafka</code> contains the native Python client for Kafka. The <code>requests</code> library we use to <code>HTTP POST</code> requests to our ksqlDB server</p>
</div>
</li>
<li>
<p>Again in the same folder locate and open the file <code>main.py</code>. Analyze its content. Discuss it with your peers to make sure you understand what&#8217;s going on.<br>
In essence the application does the following:</p>
<div class="ulist">
<ul>
<li>
<p>Produce some quotes</p>
</li>
<li>
<p>Call ksqlDB via its REST API to generate a streaming query</p>
</li>
<li>
<p>Use a Kafka consumer to consume the lowercase quotes produced by the streaming query</p>
</li>
</ul>
</div>
</li>
<li>
<p>Create the <code>quotes</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic quotes</strong></pre>
</div>
</div>
</li>
<li>
<p>Use <code>pip3</code> to install the requirements.</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>pip3 install -r requirements.txt</strong></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
If you get an error installing the Python packages, run <code>pip3 install --upgrade pip</code>. The upgrade will fix the error when you run <code>pip3 install -r requirements.txt</code> again.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Run the Python client with the following command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>python3 main.py</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should see an output similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code>&gt;&gt;&gt; Starting Python Kafka Client...
------ Writing quotes to topic 'quotes' ------
*** writing: Kafka enables the Confluent Streaming Platform
*** writing: Confluent offers a Streaming Platform powered by Kafka
*** writing: Kafka Streams are cool
*** writing: Streaming allows for real-time processing of information
*** writing: I love Kafka
------ done writing quotes ------

--------- Posting to KSQL Server ---------

200, [{"@type":"currentStatus","statementText":"CREATE STREAM quotes_orig (line STRING) WITH(KAFKA_TOPIC='quotes', VALUE_FORMAT='DELIMITED');","commandId":"stream/`QUOTES_ORIG`/create","commandStatus":{"status":"SUCCESS","message":"Stream created"},"commandSequenceNumber":2,"warnings":[]}],

200, [{"@type":"currentStatus","statementText":"CREATE STREAM QUOTES_LOWER WITH (KAFKA_TOPIC='QUOTES_LOWER', PARTITIONS=1, REPLICAS=1) AS SELECT LCASE(QUOTES_ORIG.LINE) KSQL_COL_0\nFROM QUOTES_ORIG QUOTES_ORIG\nEMIT CHANGES;","commandId":"stream/`QUOTES_LOWER`/create","commandStatus":{"status":"SUCCESS","message":"Created query with ID CSAS_QUOTES_LOWER_0"},"commandSequenceNumber":4,"warnings":[]}],

--------- done posting to KSQL Server -----------

&gt;&gt;&gt; Starting Python Kafka Client...
------ Reading from topic 'QUOTES_LOWER' ------
Received message: kafka enables the confluent streaming platform
Received message: confluent offers a streaming platform powered by kafka
Received message: kafka streams are cool
Received message: streaming allows for real-time processing of information
Received message: i love kafka
&lt;&lt;&lt; Ending Python Kafka Client...</code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_using_the_ksqldb_cli">Using the ksqlDB CLI</h4>
<div class="olist arabic">
<ol class="arabic" start="9">
<li>
<p>Enter the ksqlDB CLI with this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>ksql http://ksqldb-server:8088</strong></pre>
</div>
</div>
</li>
<li>
<p>Show all streams:</p>
<div class="listingblock">
<div class="content">
<pre>$ ksql&gt; <strong>SHOW STREAMS;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should see this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Stream Name         | Kafka Topic                 | Key Format | Value Format | Windowed
----------------------------------------------------------------------------------------
KSQL_PROCESSING_LOG | default_ksql_processing_log | KAFKA      | JSON         | false
QUOTES_LOWER        | QUOTES_LOWER                | KAFKA      | DELIMITED    | false
QUOTES_ORIG         | quotes                      | KAFKA      | DELIMITED    | false
----------------------------------------------------------------------------------------</pre>
</div>
</div>
</li>
<li>
<p>Use the ksqlDB CLI Print command to list the content of the topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ ksql&gt; <strong>PRINT 'quotes' FROM BEGINNING;</strong>
Key format: ¯_(ツ)_/¯ - no data processed
Value format: KAFKA_STRING
rowtime: 2022/04/20 14:15:39.093 Z, key: &lt;null&gt;, value: Kafka enables the Confluent Streaming Platform, partition: 0
rowtime: 2022/04/20 14:15:39.093 Z, key: &lt;null&gt;, value: Confluent offers a Streaming Platform powered by Kafka, partition: 0
rowtime: 2022/04/20 14:15:39.093 Z, key: &lt;null&gt;, value: Kafka Streams are cool, partition: 0
rowtime: 2022/04/20 14:15:39.093 Z, key: &lt;null&gt;, value: Streaming allows for real-time processing of information, partition: 0
rowtime: 2022/04/20 14:15:39.093 Z, key: &lt;null&gt;, value: I love Kafka, partition: 0</pre>
</div>
</div>
<div class="paragraph">
<p>Press <code>Ctrl+C</code> to end the query.</p>
</div>
</li>
<li>
<p>Set the starting point of your queries to <code>earliest</code>:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SET 'auto.offset.reset' = 'earliest';</strong></pre>
</div>
</div>
</li>
<li>
<p>Set the column width to <code>50</code>:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SET CLI COLUMN-WIDTH 50;</strong></pre>
</div>
</div>
</li>
<li>
<p>List the content of both streams:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM quotes_orig EMIT CHANGES LIMIT 5;</strong>
+--------------------------------------------------------------------------------+
|LINE                                                                            |
+--------------------------------------------------------------------------------+
|Kafka enables the Confluent Streaming Platform                                  |
|Confluent offers a Streaming Platform powered by Kafka                          |
|Kafka Streams are cool                                                          |
|Streaming allows for real-time processing of information                        |
|I love Kafka                                                                    |
Limit Reached
Query terminated</pre>
</div>
</div>
<div class="paragraph">
<p>and</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM quotes_lower EMIT CHANGES LIMIT 3;</strong>
+--------------------------------------------------------------------------------+
|KSQL_COL_0                                                                      |
+--------------------------------------------------------------------------------+
|kafka enables the confluent streaming platform                                  |
|confluent offers a streaming platform powered by kafka                          |
|kafka streams are cool                                                          |
Limit Reached
Query terminated
ksql&gt;</pre>
</div>
</div>
</li>
<li>
<p>Quit the ksqlDB CLI with <code>Ctrl+D</code></p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_5">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="16">
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_6">Conclusion</h4>
<div class="paragraph">
<p>In this exercise we have created a Kafka client application in Python that uses the ksqlDB REST API to access the ksqlDB functionality. The Python client executed the following tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>write entries to an existing topic <code>quotes</code></p>
</li>
<li>
<p>post a query to ksqlDB server to create a stream from the topic <code>quotes</code></p>
</li>
<li>
<p>post another query to ksqlDB server to create a stream <code>quotes_lower</code> containing the quotes from the topic <code>quotes</code> all in lower case</p>
</li>
<li>
<p>read from the topic <code>quotes_lower</code> and output the messages to the screen</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_c_creating_connectors_with_ksqldb">c. Creating connectors with ksqlDB</h3>
<div class="paragraph">
<p>The idea of this exercise is to show how to create connectors using ksqlDB server. In the lab, you will create two JDBC source connectors to import a table from PostgresDB to Kafka. One connector will have a simple configuration just to move the data to Kafka; and the other connector will be configured with SMTs (Single Message Transforms) to transform the data while is imported to Kafka.<br>
The architecture of this lab is composed by a Zookeeper cluster, Kafka cluster, ksqlDB cluster, Kafka Connect cluster and PostgresDB. ksqlDB is configured to communicate to Connect, so you can manage the connectors in Connect using KSQL queries.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_6">Prerequisites</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a terminal window and navigate to the folder <code>~/confluent-streams</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams</strong></pre>
</div>
</div>
</li>
<li>
<p>Run the application with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka schema-registry ksqldb-server control-center connect postgres</strong></pre>
</div>
</div>
</li>
<li>
<p>Create the <code>postgres-no-key-passengers</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic postgres-no-key-passengers</strong></pre>
</div>
</div>
</li>
<li>
<p>Create the <code>postgres-with-key-passengers</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic postgres-with-key-passengers</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_preparing_postgres_database">Preparing Postgres Database</h4>
<div class="olist arabic">
<ol class="arabic" start="5">
<li>
<p>First connect to the Postgres database:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>psql -h postgres -U postgres</strong>
psql (11.2)
Type "help" for help.

postgres=#</pre>
</div>
</div>
</li>
<li>
<p>At the postgres prompt use a SQL statement to create a new table with the name <code>passengers</code>. Run:</p>
<div class="listingblock">
<div class="content">
<pre>postgres=# <strong>create table passengers("Id" int primary key not null, "Name" varchar (100), "Email" varchar (255), "Age" integer, "Travel_to" varchar (255), "Payment" integer, "Travel_date" date);</strong></pre>
</div>
</div>
</li>
<li>
<p>Insert some data to the table by running:</p>
<div class="listingblock">
<div class="content">
<pre>postgres=# <strong>INSERT INTO "passengers" ("Id", "Name", "Email", "Age", "Travel_to", "Payment", "Travel_date")
VALUES
(1, 'Jack', 'jack12@gmail.com', 20, 'Paris', 79000, '2018-1-1'),
(2, 'Anna', 'anna@gmail.com', 19, 'NewYork', 405000, '2019-10-3'),
(3, 'Wonder', 'wonder2@yahoo.com', 32, 'Sydney', 183000, '2012-8-5'),
(4, 'Stacy', 'stacy78@hotmail.com', 28, 'Maldives', 29000, '2017-6-9'),
(5, 'Stevie', 'stevie@gmail.com', 49, 'Greece', 56700, '2021-12-12'),
(6, 'Harry', 'harry@gmail.com', 22, 'Hogwarts', 670000, '2020-1-17'),
(7, 'Max', 'max@gmail.com', 19, 'Paris', 61000, '2022-4-27');</strong></pre>
</div>
</div>
</li>
<li>
<p>Use a SQL select statement to view the contents of the <code>passengers</code> table:</p>
<div class="listingblock">
<div class="content">
<pre>postgres=# <strong>select * from passengers;</strong>
 Id |  Name  |        Email        | Age | Travel_to | Payment | Travel_date
----+--------+---------------------+-----+-----------+---------+-------------
  1 | Jack   | jack12@gmail.com    |  20 | Paris     |   79000 | 2018-01-01
  2 | Anna   | anna@gmail.com      |  19 | NewYork   |  405000 | 2019-10-03
  3 | Wonder | wonder2@yahoo.com   |  32 | Sydney    |  183000 | 2012-08-05
  4 | Stacy  | stacy78@hotmail.com |  28 | Maldives  |   29000 | 2017-06-09
  5 | Stevie | stevie@gmail.com    |  49 | Greece    |   56700 | 2021-12-12
  6 | Harry  | harry@gmail.com     |  22 | Hogwarts  |  670000 | 2020-01-17
  7 | Max    | max@gmail.com       |  19 | Paris     |   61000 | 2022-04-27</pre>
</div>
</div>
</li>
<li>
<p>Stop the query by typing <code>:q</code>.</p>
</li>
<li>
<p>Exit <code>psql</code> by pressing <code>Ctrl+D</code>.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_install_the_kafka_connect_jdbc_connector">Install the Kafka Connect JDBC Connector</h4>
<div class="paragraph">
<p>We use the Kafka Connect JDBC connector in this exercise so we need to install the connector JAR file in Kafka Connect before we can create a JDBC connector.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="11">
<li>
<p>Install the connector:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose exec -u root connect confluent-hub install confluentinc/kafka-connect-jdbc:10.0.0</strong>
The component can be installed in any of the following Confluent Platform installations:
  1. / (installed rpm/deb package)
  2. / (where this tool is installed)
Choose one of these to continue the installation (1-2):</pre>
</div>
</div>
</li>
<li>
<p>Type <code>1</code> and press <strong>Enter</strong>.</p>
</li>
<li>
<p>At the prompt, type <code>y</code> and press <strong>Enter</strong>.</p>
<div class="listingblock">
<div class="content">
<pre>Do you want to install this into /usr/share/confluent-hub-components? (yN)</pre>
</div>
</div>
</li>
<li>
<p>At the prompt, type <code>y</code> and press <strong>Enter</strong>.</p>
<div class="listingblock">
<div class="content">
<pre>Component's license:
Confluent Community License
https://www.confluent.io/confluent-community-license
I agree to the software license agreement (yN)</pre>
</div>
</div>
</li>
<li>
<p>At the prompt, type <code>y</code> and press <strong>Enter</strong>.</p>
<div class="listingblock">
<div class="content">
<pre>Downloading component Kafka Connect JDBC 10.0.0, provided by Confluent, Inc. from Confluent Hub and installing into /usr/share/confluent-hub-components
Detected Worker's configs:
  1. Standard: /etc/kafka/connect-distributed.properties
  2. Standard: /etc/kafka/connect-standalone.properties
  3. Standard: /etc/schema-registry/connect-avro-distributed.properties
  4. Standard: /etc/schema-registry/connect-avro-standalone.properties
  5. Used by Connect process with PID : /etc/kafka-connect/kafka-connect.properties
Do you want to update all detected configs? (yN)</pre>
</div>
</div>
<div class="paragraph">
<p>The installation completes.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>Adding installation directory to plugin path in the following files:
  /etc/kafka/connect-distributed.properties
  /etc/kafka/connect-standalone.properties
  /etc/schema-registry/connect-avro-distributed.properties
  /etc/schema-registry/connect-avro-standalone.properties
  /etc/kafka-connect/kafka-connect.properties

Completed</pre>
</div>
</div>
</li>
<li>
<p>To complete the installation, we need to restart the <code>connect</code> container:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose restart connect</strong></pre>
</div>
</div>
</li>
<li>
<p>Verify that the Connect Worker successfully restarted prior to continuing to the next step:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose logs connect | grep -i "INFO .* Finished starting connectors and tasks"</strong>
connect | [2022-05-13 19:46:25,684] INFO [Worker clientId=connect-1, groupId=connect] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)
connect | [2022-05-13 20:04:11,726] INFO [Worker clientId=connect-1, groupId=connect] Finished starting connectors and tasks (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1236)</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Repeat this command until the <strong>Finished starting connectors and tasks</strong> message appears.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_create_basic_connector_using_ksqldb">Create basic connector using ksqlDB</h4>
<div class="olist arabic">
<ol class="arabic" start="18">
<li>
<p>Go to Control Center <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a> and click on <strong>ksqlDB</strong> at the left pane.</p>
</li>
<li>
<p>Click on the ksqlDB application called <strong>ksqldb</strong>.</p>
</li>
<li>
<p>Run the following KSQL query in the Editor box to create a JDBC source connector:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_NO_KEY WITH (
    'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector',
    'connection.url'= 'jdbc:postgresql://postgres:5432/postgres',
    'connection.user'= 'postgres',
    'table.whitelist'= 'passengers',
    'mode'= 'incrementing',
    'incrementing.column.name'= 'Id',
    'topic.prefix'= 'postgres-no-key-'
);
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
This connector will import each row as a single message in Kafka. By default, this connector puts all values of each row in the Kafka&#8217;s message value, leaving the message key <code>null</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Wait for a few seconds until the new connector starts importing the data. Navigate to <strong>Topics</strong> and select <strong>postgres-no-key-passengers</strong> topic.</p>
</li>
<li>
<p>In the <strong>Overview</strong> tab, there is a table with one partition at the bottom of the page. In the column <strong>Offset</strong>, you should see <code>Start = 0</code> and <code>End = 7</code> indicating that seven messages have been produced to this topic.</p>
</li>
<li>
<p>In the <strong>Schema</strong> tab, you can see the AVRO schema automatically generated by the JDBC connector based on the metadata of the Postgres table.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_create_connector_with_smts_using_ksqldb">Create connector with SMTs using ksqlDB</h4>
<div class="olist arabic">
<ol class="arabic" start="24">
<li>
<p>Navigate again to <strong>ksqlDB</strong> in Control Center to create a second connector with Single Message Transforms (SMTs).</p>
</li>
<li>
<p>Run the following KSQL query in the Editor box to create a JDBC source connector with SMTs:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
</pre></td><td class="code"><pre>CREATE SOURCE CONNECTOR JDBC_SOURCE_POSTGRES_WITH_KEY WITH (
    'connector.class'= 'io.confluent.connect.jdbc.JdbcSourceConnector',
    'connection.url'= 'jdbc:postgresql://postgres:5432/postgres',
    'connection.user'= 'postgres',
    'table.whitelist'= 'passengers',
    'mode'= 'incrementing',
    'incrementing.column.name'= 'Id',
    'topic.prefix'= 'postgres-with-key-',
    'transforms'= 'copyFieldToKey,extractKeyFromStruct,removeKeyFromValue',
    'transforms.copyFieldToKey.type'= 'org.apache.kafka.connect.transforms.ValueToKey',
    'transforms.copyFieldToKey.fields'= 'Id',
    'transforms.extractKeyFromStruct.type'= 'org.apache.kafka.connect.transforms.ExtractField$Key',
    'transforms.extractKeyFromStruct.field'= 'Id',
    'transforms.removeKeyFromValue.type'= 'org.apache.kafka.connect.transforms.ReplaceField$Value',
    'transforms.removeKeyFromValue.blacklist'= 'Id',
    'key.converter' = 'org.apache.kafka.connect.converters.IntegerConverter'
);
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Explanation about the new code:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>There are three transforms to set the key to the value of the <code>Id</code> field. They run in the order defined by <code>transforms</code>:</p>
<div class="ulist">
<ul>
<li>
<p><code>copyFieldToKey</code> sets the key to a struct containing the <code>Id</code> field from the value.</p>
</li>
<li>
<p><code>extractKeyFromStruct</code> sets the key to just the <code>Id</code> field of the struct set by the previous step.</p>
</li>
<li>
<p><code>removeKeyFromValue</code> removes the <code>Id</code> field from the message value, as it’s now stored in the message key.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Last line - the key is an integer so we override the default serialization (<code>StringConverter</code>) and instead use the <code>IntegerConverter</code> for the key field.</p>
</li>
</ol>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Wait for a few seconds until the new connector starts importing the data. Navigate to <strong>Topics</strong> and select <strong>postgres-with-key-passengers</strong> topic.</p>
</li>
<li>
<p>In the <strong>Overview</strong> tab, you should see <code>Start = 0</code> and <code>End = 7</code> in the column <strong>Offset</strong>, indicating that seven messages have been produced to this topic.</p>
</li>
<li>
<p>In the <strong>Schema</strong> tab, you can see the AVRO schema automatically generated by the JDBC connector. Note that the field <code>Id</code> is not present in the value schema, since it has been moved to the message key by the SMTs.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_read_topics_using_ksqldb">Read topics using ksqlDB</h4>
<div class="paragraph">
<p>In this part, we will read the data imported to <code>postgres-no-key-passengers</code> and <code>postgres-with-key-passengers</code> topics.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="29">
<li>
<p>Navigate to <strong>ksqlDB</strong> in Control Center.</p>
</li>
<li>
<p>Let’s now inspect the data on the Kafka topic <code>postgres-no-key-passengers</code>. ksqlDB’s <code>PRINT</code> command will show the contents of a topic. Run this query:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>PRINT 'postgres-no-key-passengers' FROM BEGINNING;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Note that <code>key: &lt;null&gt;</code> and the field <code>Id</code> is within the value.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Click <code>Stop</code> to finish the query.</p>
</li>
<li>
<p>Now, read the messages in <code>postgres-with-key-passengers</code> topic by running this query:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>PRINT 'postgres-with-key-passengers' FROM BEGINNING;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Check the results. In this case the <code>key</code> is the field <code>Id</code> value.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Click <code>Stop</code> to finish the query.</p>
</li>
</ol>
</div>
<div class="sect4">
<h5 id="_optional_extra_content">Optional: Extra Content</h5>
<div class="paragraph">
<p>You can use the <code>kafka-avro-console-consumer</code> to read AVRO data using the Terminal. This tool uses the Avro converter with the Schema Registry in order to properly read the Avro data schema and write the messages to standard output (console) in JSON format.</p>
</div>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Open a Terminal window.</p>
</li>
<li>
<p>Run this command to read the messages from <code>postgres-no-key-passengers</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-avro-console-consumer \
    --bootstrap-server kafka:9092 \
    --topic postgres-no-key-passengers \
    --from-beginning \
    --property schema.registry.url=http://schema-registry:8081 \
    --property print.key=true \
    --property key.separator=" | "</strong>
null | {"Id":1,"Name":{"string":"Jack"},"Email":{"string":"jack12@gmail.com"},"Age":{"int":20},"Travel_to":{"string":"Paris"},"Payment":{"int":79000},"Travel_date":{"int":17532}}
null | {"Id":2,"Name":{"string":"Anna"},"Email":{"string":"anna@gmail.com"},"Age":{"int":19},"Travel_to":{"string":"NewYork"},"Payment":{"int":405000},"Travel_date":{"int":18172}}
null | {"Id":3,"Name":{"string":"Wonder"},"Email":{"string":"wonder2@yahoo.com"},"Age":{"int":32},"Travel_to":{"string":"Sydney"},"Payment":{"int":183000},"Travel_date":{"int":15557}}
...</pre>
</div>
</div>
</li>
<li>
<p>Press <code>Ctrl+C</code> to stop the <code>kafka-avro-console-consumer</code>.</p>
</li>
<li>
<p>Run this command to read the messages from <code>postgres-no-key-passengers</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-avro-console-consumer \
    --bootstrap-server kafka:9092 \
    --topic postgres-with-key-passengers \
    --from-beginning \
    --property schema.registry.url=http://schema-registry:8081 \
    --property print.key=true \
    --property key.separator=" | " \
    --key-deserializer org.apache.kafka.common.serialization.IntegerDeserializer</strong>
1 | {"Name":{"string":"Jack"},"Email":{"string":"jack12@gmail.com"},"Age":{"int":20},"Travel_to":{"string":"Paris"},"Payment":{"int":79000},"Travel_date":{"int":17532}}
2 | {"Name":{"string":"Anna"},"Email":{"string":"anna@gmail.com"},"Age":{"int":19},"Travel_to":{"string":"NewYork"},"Payment":{"int":405000},"Travel_date":{"int":18172}}
3 | {"Name":{"string":"Wonder"},"Email":{"string":"wonder2@yahoo.com"},"Age":{"int":32},"Travel_to":{"string":"Sydney"},"Payment":{"int":183000},"Travel_date":{"int":15557}}
...</pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Note we are overriding the default <code>--key-deserializer</code> with <code>IntegerDeserializer</code>, since the key was serialized as <code>Integer</code> by the JDBC connector.<br>
Default deserializer is <code>StringDeserializer</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Press <code>Ctrl+C</code> to stop the <code>kafka-avro-console-consumer</code>.</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect3">
<h4 id="_manage_connectors_using_ksqldb">Manage connectors using ksqlDB</h4>
<div class="olist arabic">
<ol class="arabic" start="34">
<li>
<p>Check all connectors that you have created. Run this query in the Editor box in <strong>ksqlDB</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>SHOW CONNECTORS;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="paragraph">
<p>In the result, you should see two connectors running:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>JDBC_SOURCE_POSTGRES_NO_KEY</p>
</li>
<li>
<p>JDBC_SOURCE_POSTGRES_WITH_KEY</p>
</li>
</ul>
</div>
</li>
<li>
<p>Get extra information about a connector using the function <code>DESCRIBE CONNECTOR</code>. Run this query:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>DESCRIBE CONNECTOR JDBC_SOURCE_POSTGRES_WITH_KEY;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</li>
<li>
<p>Use the function <code>DROP CONNECTOR</code> to delete a connector from the Connect cluster. The topics associated with this connector are not deleted by this command. Run this query to delete <code>JDBC_SOURCE_POSTGRES_NO_KEY</code> connector:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>DROP CONNECTOR JDBC_SOURCE_POSTGRES_NO_KEY;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</li>
<li>
<p>Check the connector was deleted by running again:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="attributes"source"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>SHOW CONNECTORS;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_6">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="38">
<li>
<p>Shut down all Docker containers with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_7">Conclusion</h4>
<div class="paragraph">
<p>In this lab, you have learnt how to create connectors using ksqlDB, how to read data from Kafka topics using ksqlDB and how to manage connectors using ksqlDB. Additionally, you have learnt how to install a connector in a Kafka Connect cluster and how to use the tool <code>kafka-avro-console-consumer</code> to read AVRO data from Kafka using your console.</p>
</div>
<div class="paragraph">
<p> 
 
 </p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_04_using_ksqldb">Lab 04 Using ksqlDB</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_a_using_ksqldb">a. Using ksqlDB</h3>
<div class="paragraph">
<p>In this exercise, we will explore how to apply ksqlDB&#8217;s mapping and filtering capabilities to an application that processes real-time temperature data. MQTT and Internet of Things data are perfect for real-time processing with Kafka Streams and ksqlDB. ksqlDB is an especially good choice for many of these applications because of its simplicity.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_7">Prerequisites</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_preparing_the_kafka_cluster_2">Preparing the Kafka Cluster</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Open a terminal window and navigate to the folder `~/confluent-streams</p>
<div class="listingblock">
<div class="content">
<pre>   $ <strong>cd ~/confluent-streams</strong></pre>
</div>
</div>
</li>
<li>
<p>Run a Kafka cluster and a ksqlDB server using this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka ksqldb-server</strong>
Creating network "confluent-streams_kafka-net" with the default driver
Creating ksqldb-server     ... done
Creating zookeeper       ... done
Creating kafka           ... done</pre>
</div>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
Wait a couple of minutes until the cluster is ready.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Check the status with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose ps</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_creating_data">Creating Data</h4>
<div class="olist arabic">
<ol class="arabic" start="4">
<li>
<p>Create the <code>stations</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic stations \
    --config cleanup.policy=compact</strong></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
We do this compaction only for illustration. Normally the temperature reading stations wouldn&#8217;t change too frequently to warrant compaction.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Create a list of stations. Here we use the kafka-console-producer command line tool to send records in to the Kafka cluster.</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cat &lt;&lt; EOF | kafka-console-producer \
       --bootstrap-server kafka:9092 \
       --property "parse.key=true" \
       --property "key.separator=:" \
       --topic stations
1:Mombasa,Kenya
2:Nairobi,Kenya
3:Mogadishu,Somalia
4:Dar es Salaam,Tanzania
5:Pretoria,South Africa
6:Cape Town,South Africa
7:Bloemfontein,South Africa
8:Diani,Kenya
9:Embu,Kenya
10:Johannesburg,South Africa
EOF</strong>

&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</pre>
</div>
</div>
</li>
<li>
<p>Double check that the list of stations has been created:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --max-messages 7 \
    --topic stations \
    --property print.key=true \
    --property key.separator=":"</strong>
1:Mombasa,Kenya
2:Nairobi,Kenya
3:Mogadishu,Somalia
4:Dar es Salaam,Tanzania
5:Pretoria,South Africa
6:Cape Town,South Africa
7:Bloemfontein,South Africa
8:Diani,Kenya
9:Embu,Kenya
10:Johannesburg,South Africa
Processed a total of 10 messages</pre>
</div>
</div>
</li>
<li>
<p>Create the <code>temperatures</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic temperatures</strong></pre>
</div>
</div>
</li>
<li>
<p>Create a list of temperature readings, again using the kafka-console-producer command line tool.</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cat &lt;&lt; EOF | kafka-console-producer \
       --bootstrap-server kafka:9092 \
       --topic temperatures
6,2,18.5
5,1,23
20,5,21.3
15,4,35.5
17,4,34.5
9,2,18
24,7,17
13,3,33
19,5,21
25,7,18
14,3,32
4,1,24.5
7,2,16.5
23,7,16
3,1,26
12,3,31
2,1,25.5
22,7,17
1,1,25
18,4,37.5
11,2,18
10,2,17.5
16,4,35
8,2,16
21,6,23
EOF</strong>
&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_ksqldb_cli_2">Running the ksqlDB CLI</h4>
<div class="olist arabic">
<ol class="arabic" start="9">
<li>
<p>Open a new terminal window and run the ksqlDB CLI using this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>ksql http://ksqldb-server:8088</strong></pre>
</div>
</div>
</li>
<li>
<p>Define that streams should be read from beginning:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SET 'auto.offset.reset' = 'earliest';</strong>
Successfully changed local property 'auto.offset.reset' from 'null' to 'earliest'</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_mapping_and_filtering">Mapping and Filtering</h4>
<div class="olist arabic">
<ol class="arabic" start="11">
<li>
<p>Create a table from the topic <code>stations</code>:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>CREATE TABLE weather_stations(
        id VARCHAR PRIMARY KEY,
        name VARCHAR,
        country VARCHAR
    )
    WITH(kafka_topic='stations', value_format='DELIMITED');</strong>

 Message
---------------
 Table created
---------------</pre>
</div>
</div>
</li>
<li>
<p>Run a simple query against this new table:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM weather_stations EMIT CHANGES LIMIT 5;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>you should see something like this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>+--------+------------------+-----------------+
|ID      |NAME              |COUNTRY
+--------+------------------+-----------------+
|1       |Mombasa           |Kenya            |
|2       |Nairobi           |Kenya            |
|3       |Mogadishu         |Somalia          |
|4       |Dar es Salaam     |Tanzania         |
|5       |Pretoria          |South Africa     |
Limit Reached
Query terminated</pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You can also peek into the <code>stations</code> topic using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>PRINT 'stations' FROM BEGINNING;</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Press <code>Ctrl+C</code> to stop the above query</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Now let&#8217;s only output stations in Kenya:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT * FROM weather_stations
      WHERE country='Kenya'
      EMIT CHANGES;</strong>
+--------+------------------+-----------------+
|ID      |NAME              |COUNTRY
+--------+------------------+-----------------+
|1       |Mombasa           |Kenya            |
|2       |Nairobi           |Kenya            |
|8       |Diani             |Kenya            |
|9       |Embu              |Kenya            |</pre>
</div>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Press <code>Ctrl+C</code> to end the query.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>To show only stations whose name starts with <strong>M</strong> and output the country in all caps use:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT id, name, UCASE(country) AS country
    FROM weather_stations
    WHERE name LIKE 'M%'
    EMIT CHANGES;</strong>
+--------+------------------+-----------------+
|ID      |NAME              |COUNTRY
+--------+------------------+-----------------+
|1       |Mombasa           |Kenya            |
|2       |Mogadishu         |Somalia          |</pre>
</div>
</div>
<div class="paragraph">
<p>Press <code>Ctrl+C</code> to end the query.</p>
</div>
</li>
<li>
<p>Create a stream from the topic <code>temperatures</code>:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>CREATE STREAM mytemperatures(
        id INTEGER,
        station_id VARCHAR,
        temp DOUBLE
    )
    WITH(kafka_topic='temperatures', value_format='DELIMITED');</strong></pre>
</div>
</div>
</li>
<li>
<p>The temperatures are in degree Celsius. To output them in degree Fahrenheit use this:</p>
<div class="listingblock">
<div class="content">
<pre>ksql&gt; <strong>SELECT id, station_id,
             temp AS temp_in_C,
             temp*9/5+32 as temp_in_F
      FROM mytemperatures
      EMIT CHANGES
      LIMIT 10;</strong>
+----------------+---------------+------------------+------------------+
|ID              |STATION_ID     |TEMP_IN_C         |TEMP_IN_F         |
+----------------+---------------+------------------+------------------+
|6               |2              |18.5              |65.3              |
|5               |1              |23.0              |73.4              |
|20              |5              |21.3              |70.34             |
|15              |4              |35.5              |95.9              |
|17              |4              |34.5              |94.1              |
|9               |2              |18.0              |64.4              |
|24              |7              |17.0              |62.6              |
|13              |3              |33.0              |91.4              |
|19              |5              |21.0              |69.8              |
|25              |7              |18.0              |64.4              |
Limit Reached
Query terminated</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_7">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="17">
<li>
<p>Exit the <strong>ksqlDB CLI</strong> by pressing <code>Ctrl+D</code>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_8">Conclusion</h4>
<div class="paragraph">
<p>In this exercise, we have explored the various capabilities that ksqlDB offers us in a easy and convenient way. We have learned that the syntax of ksqlDB SQL strongly resembles that of ANSI SQL. We have explored stateless functions such as mapping and filtering.</p>
</div>
<div class="paragraph">
<p> 
 
 </p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_06_windowing_aggregations">Lab 06 Windowing &amp; Aggregations</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In this exercise you will create a Kafka Streams application and a ksqlDB application to sessionize click data from a website. You will be organizing the user&#8217;s click behaviour data collected from a website using sessions window.</p>
</div>
<div class="paragraph">
<p>You can choose to work on either Kafka Streams or ksqlDB lab or both.</p>
</div>
<div class="sect2">
<h3 id="_a_windowing_aggregations">a. Windowing &amp; Aggregations</h3>
<div class="sect3">
<h4 id="_prerequisites_8">Prerequisites</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_writing_the_data_to_kafka">Writing the data to Kafka</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Use the command in the table below to navigate to the project folder for your language:</p>
<div class="listingblock">
<div class="content">
<pre><strong>cd ~/confluent-streams/labs/windowing</strong></pre>
</div>
</div>
</li>
<li>
<p>If your Kafka cluster is not already running, start it with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka schema-registry ksqldb-server control-center</strong></pre>
</div>
</div>
</li>
<li>
<p>Create one input topic called <code>clicks-topic</code> and two output topics called <code>window-streams</code> and <code>window-ksql</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic clicks-topic</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic window-streams</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic window-ksql</strong></pre>
</div>
</div>
</li>
<li>
<p>Start producing data to Kafka using the Java producer <code>clicks-producer</code>:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Navigate to the producer folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ./protobuf-java-producer</strong></pre>
</div>
</div>
</li>
<li>
<p>Use <code>gradle</code> to generate the PROTOBUF class:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>./gradlew build</strong></pre>
</div>
</div>
</li>
<li>
<p>Run this command to start the Java producer:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>./gradlew run</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>We have the clicks data in <code>clicks-topic</code>. Now, we are going to use <strong>Kafka Streams</strong> and <strong>ksqlDB</strong> to do the same operation (use session windows to count the number of clicks for each IP address).</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_using_session_windows_in_kafka_streams">Using Session Windows in Kafka Streams</h4>
<div class="olist arabic">
<ol class="arabic" start="6">
<li>
<p>Open a new Terminal window and run the following command to open <strong>count-streams-app</strong> in Visual Studio Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>code ~/confluent-streams/labs/windowing/count-streams-app</strong></pre>
</div>
</div>
</li>
<li>
<p>In VS Code, navigate to <code>src/main/java/io/confluent/training/app/</code> and open the Java file <code>StreamsApp.java</code></p>
</li>
<li>
<p>Locate in the code the <code>TO-DO</code> lines and <code>{{ WRITE-MISSING-CODE }}</code> markers. Try to write yourself the missing code by checking the documentation:</p>
<div class="ulist">
<ul>
<li>
<p>TO-DO 1 - <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#creating-source-streams-from-ak" target="_blank" rel="noopener">documentation</a>: create a KStream from the "clicks-topic" topic and configure the Key-Serde and Value-Serde that can read the String key, and Clicks value.</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre>final KStream&lt;String, ClicksProtos.Clicks&gt; clicks = <strong>builder.stream("clicks-topic", Consumed.with(Serdes.String(), clicksSerde))</strong>;</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>TO-DO 2 - <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#stateless-transformations" target="_blank" rel="noopener">documentation</a>: group by key the KStream "clicks".</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre>final KGroupedStream&lt;String, ClicksProtos.Clicks&gt; clicksGrouped = <strong>clicks.groupByKey()</strong>;</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>TO-DO 3 - <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#session-windows" target="_blank" rel="noopener">documentation</a>: apply a Session Window of 5 minutes with a Grace period of 30 seconds to the KGroupedStream "clicksGrouped" and apply a count to get the number of clicks per IP per Session Window.</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre>final KTable&lt;Windowed&lt;String&gt;, Long&gt; clicksCount = <strong>clicksGrouped
&#160;&#160;&#160;&#160;&#160;&#160;&#160;.windowedBy(SessionWindows.with(Duration.ofMinutes(5)).grace(Duration.ofSeconds(30)))
&#160;&#160;&#160;&#160;&#160;&#160;&#160;.count()</strong>;</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>TO-DO 4 - <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#stateless-transformations" target="_blank" rel="noopener">documentation</a>: convert the KTable "clicksCount" into a KStream.</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre>final KStream&lt;Windowed&lt;String&gt;, Long&gt; clicksCountStream = <strong>clicksCount.toStream()</strong>;</pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>TO-DO 5 - <a href="https://docs.confluent.io/platform/current/streams/developer-guide/dsl-api.html#writing-streams-back-to-ak" target="_blank" rel="noopener">documentation</a>: produce the data of the KStream "clicksCountStreamModified" to the topic "window-streams" selecting the appropriate Serdes for key and value.</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre><strong>clicksCountStreamModified.to("window-streams", Produced.with(Serdes.String(), clicksCountSerde))</strong>;</pre>
</div>
</div>
</div>
</details>
</li>
</ul>
</div>
</li>
<li>
<p>After completing all the TO-DO&#8217;s, start the Kafka Streams application by clicking in the top menu: <strong>Run</strong> &#8594; <strong>Start Debugging</strong>.</p>
</li>
<li>
<p>Let the application run for a few seconds and check the results in Confluent Control Center <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a>:</p>
<div class="ulist">
<ul>
<li>
<p>Go to <strong>Topics</strong> and select <strong>window-streams</strong> topic</p>
</li>
<li>
<p>Click on the tab <strong>Messages</strong></p>
</li>
<li>
<p>In the box at the top where says <code>offset</code> with a magnifier, type <code><strong>0</strong></code>. A dropdown list will appear, then select <code><strong>0/Partition: 0</strong></code></p>
</li>
<li>
<p>Wait for a few seconds and you will see the output from the Kafka Streams app</p>
</li>
</ul>
</div>
</li>
<li>
<p>Check the results and try to answer the following questions:</p>
<div class="ulist">
<ul>
<li>
<p>How many different IP addresses are there?</p>
</li>
<li>
<p>How many different sessions are there?</p>
</li>
<li>
<p>Which IP has the highest number of clicks?</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
It might be easier to answer those questions when "Table View" is selected for the messages in top right corner in Control Center, as compared to the "Cards View".
</td>
</tr>
</table>
</div>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_using_session_windows_in_ksqldb">Using Session Windows in ksqlDB</h4>
<div class="paragraph">
<p>In this section we are going to apply the same transformations but using ksqlDB, so you can easily compare the differences and similarities between both approaches.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="12">
<li>
<p>Go to Control Center <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a> and click on <strong>ksqlDB</strong> at the left pane.</p>
</li>
<li>
<p>Click on the ksqlDB application called <strong>ksqldb</strong>. This is your ksqlDB cluster formed by one server which is the ksqldb-server Docker container.</p>
</li>
<li>
<p>Set the following property from dropdown to ensure that you’re reading from the beginning:</p>
<div class="listingblock">
<div class="content">
<pre><strong>auto.offset.reset = Earliest</strong></pre>
</div>
</div>
</li>
<li>
<p>Now you will create and populate a new stream <code>clicks</code> with the data from <code>clicks-topic</code>. An important characteristic of this data is the timestamp because this is what drives the session window. ksqlDB can use either the Kafka message timestamp, or a field from the message value as the timestamp. In this example we’ll use the latter—the event time as stored in the <code>timestamp</code> field of the message value. Copy and paste the following code in the Editor box, then click on <strong>Run query</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
</pre></td><td class="code"><pre>CREATE STREAM clicks
   WITH (KAFKA_TOPIC='clicks-topic',
   TIMESTAMP='timestamp',
   VALUE_FORMAT='PROTOBUF');
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</li>
<li>
<p>Run the following non-persistent query to check that the pseudo column <code>ROWTIME</code> (system column) contains the same value in milliseconds as the message column <code>timestamp</code>:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>SELECT ROWTIME, timestamp FROM clicks EMIT CHANGES LIMIT 5;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</li>
<li>
<p>If you want to see the timestamp values in a more readable format, use the function <code>TIMESTAMPTOSTRING()</code>. Run this query:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
</pre></td><td class="code"><pre>SELECT TIMESTAMPTOSTRING(ROWTIME,'yyyy-MM-dd HH:mm:ss', 'UTC') AS ROWTIME_STR, TIMESTAMPTOSTRING(timestamp,'yyyy-MM-dd HH:mm:ss', 'UTC') AS TIMESTAMP_STR FROM clicks EMIT CHANGES LIMIT 5;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</li>
<li>
<p>Before running the <code>COUNT()</code> aggregation, configure ksqlDB to buffer the aggregates as it builds them. This makes the query feel like it responds more slowly, but it means that you get just one row per window. This makes it simpler to understand the results:</p>
<div class="ulist">
<ul>
<li>
<p>Click on <strong>+Add another field</strong></p>
</li>
<li>
<p>Type: <strong>ksql.streams.cache.max.bytes.buffering</strong> = <strong>2000000</strong></p>
</li>
</ul>
</div>
</li>
<li>
<p>Using the <a href="https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/select-push-query/#session-window" target="_blank" rel="noopener">documentation</a>, write a non-persistent query with Session window to count how many clicks were made in each user session (based on IP address).
Set the Session window with a gap of 5 minutes and a grace period of 30 seconds. The output should contain these two columns:</p>
<div class="ulist">
<ul>
<li>
<p>IP</p>
</li>
<li>
<p>CLICK_COUNT</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="code"><pre>SELECT ip,
      COUNT(*) AS CLICK_COUNT
  FROM clicks
      WINDOW SESSION (5 MINUTES, GRACE PERIOD 30 SECONDS)
GROUP BY ip
EMIT CHANGES;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</div>
</details>
</li>
</ul>
</div>
</li>
<li>
<p>From the previous non-persistent query, add three more columns to the result to include the start window timestamp, the end window timestamp and the window length in milliseconds. Use the function <code>TIMESTAMPTOSTRING()</code> to print nicely the start/end window timestamps. Output columns:</p>
<div class="ulist">
<ul>
<li>
<p>IP</p>
</li>
<li>
<p>CLICK_COUNT</p>
</li>
<li>
<p>SESSION_START_TS</p>
</li>
<li>
<p>SESSION_END_TS</p>
</li>
<li>
<p>SESSION_LENGTH_MS</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
When you apply a <code>WINDOW</code> clause, ksqlDB adds two additional system columns to the data, which provide the window bounds: <code>WINDOWSTART</code> and <code>WINDOWEND</code>.
</td>
</tr>
</table>
</div>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre></td><td class="code"><pre>SELECT ip,
      COUNT(*) AS CLICK_COUNT,
      TIMESTAMPTOSTRING(WINDOWSTART,'yyyy-MM-dd HH:mm:ss', 'UTC') AS SESSION_START_TS,
      TIMESTAMPTOSTRING(WINDOWEND,'yyyy-MM-dd HH:mm:ss', 'UTC') AS SESSION_END_TS,
      WINDOWEND - WINDOWSTART AS SESSION_LENGTH_MS
FROM clicks
      WINDOW SESSION (5 MINUTES, GRACE PERIOD 30 SECONDS)
GROUP BY ip
EMIT CHANGES;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</div>
</details>
</li>
</ul>
</div>
</li>
<li>
<p>Finally, write a persistent query based on the previous query creating a new table called <code>IP_SESSIONS</code> using as backing topic <code>window-ksql</code>.</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
</pre></td><td class="code"><pre>CREATE TABLE IP_SESSIONS
       WITH (KAFKA_TOPIC='window-ksql') AS
SELECT ip,
       COUNT(*) AS CLICK_COUNT,
       TIMESTAMPTOSTRING(WINDOWSTART,'yyyy-MM-dd HH:mm:ss', 'UTC') AS SESSION_START_TS,
       TIMESTAMPTOSTRING(WINDOWEND,'yyyy-MM-dd HH:mm:ss', 'UTC') AS SESSION_END_TS,
       WINDOWEND - WINDOWSTART AS SESSION_LENGTH_MS
FROM clicks
       WINDOW SESSION (5 MINUTES, GRACE PERIOD 30 SECONDS)
GROUP BY ip;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</div>
</details>
</li>
<li>
<p>Check the results in Confluent Control Center <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a>:</p>
<div class="ulist">
<ul>
<li>
<p>Go to <strong>ksqlDB</strong> at the left pane, click on <strong>ksqldb</strong> application and click on the tab <strong>Persistent queries</strong> to analyze the query</p>
</li>
<li>
<p>Click on the tab <strong>Flow</strong> to visualize the data flow in your ksqlDB app</p>
</li>
<li>
<p>Go to <strong>Topics</strong> at the left pane and select <strong>window-ksql</strong> topic</p>
</li>
<li>
<p>Click on the tab <strong>Messages</strong></p>
</li>
<li>
<p>In the box at the top where says <code>offset</code> with a magnifier, type <code><strong>0</strong></code>. A dropdown list will appear, then select <code><strong>0/Partition: 0</strong></code></p>
</li>
<li>
<p>Wait for a few seconds and you will see the output from your ksqlDB app</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_8">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Terminate your <strong>Kafka Streams</strong> application with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_9">Conclusion</h4>
<div class="paragraph">
<p>In this exercise you created a <strong>Kafka Streams</strong> application and a <strong>ksqlDB</strong> application to sessionize click data from a website. A given user might visit a website multiple times a day, but in distinct visits; so using Session window you could automatically organize the data in sessions based on a period of inactivity.<br>
You also learned how to use the ksqlDB system columns (<code>ROWTIME</code>, <code>WINDOWSTART</code>, <code>WINDOWEND</code>) and to implement time extractors in Kafka Streams and ksqlDB queries.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_07_joins">Lab 07 Joins</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_a_joining_two_streams">a. Joining Two Streams</h3>
<div class="paragraph">
<p>In event-driven architecture, it is important to think about what event will trigger an output. Different kinds of joins will trigger outputs under different conditions. The purpose of this exercise is to experiment with the behavior of various stream-stream joins.</p>
</div>
<div class="paragraph">
<p>The streaming application itself performs a stream-stream join. It takes a string value from a "left stream" and a string value from a "right stream" and concatenates them together inside of brackets. The output is produced to a new stream. Remember that all stream-stream joins must be windowed since streams are unbounded. This application will use a tumbling window of 5 minutes.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_9">Prerequisites</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Please make sure you have prepared your lab environment as described here: &#8594;<a href="#preparing-lab">Lab Environment</a></p>
</li>
<li>
<p>If your Kafka cluster is not already running, start it with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/joining-streams</strong>
$ <strong>docker-compose up -d zookeeper kafka control-center ksqldb-server schema-registry</strong></pre>
</div>
</div>
</li>
<li>
<p>Create two input topics called <code>left-topic</code> and <code>right-topic</code> and an output topic called <code>joined-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic left-topic</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic right-topic</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic joined-topic</strong></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Remember that joins require the input topics to have the same number of partitions so that all input records with the same key, from both sides of the join, are delivered to the same stream task during processing. (called co-partitioning).
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_create_the_streaming_app">Create the Streaming App</h4>
<div class="olist arabic">
<ol class="arabic" start="4">
<li>
<p>Open another terminal and navigate to the <code>joining-streams</code> folder, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/joining-streams</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Open the file <code>build.gradle</code> and analyze its content.</p>
</li>
<li>
<p>Locate the file <code>JoinSample.java</code> in the folder <code>src/main/java/streams</code> and open it. Familiarize yourself with the code. It basically does the configuration of the <strong>Kafka Streams</strong> app but the interesting code is missing. If you would like to challenge yourself, take this time to create the streaming application logic yourself.</p>
</li>
<li>
<p>Now we define the <strong>Topology</strong> for our application. Add the following code snippet to the <code>getTopology()</code> function after the <strong>TODO</strong> comment:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nc">KStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">String</span><span class="o">&gt;</span> <span class="n">leftStream</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="na">stream</span><span class="o">(</span><span class="s">"left-topic"</span><span class="o">,</span>
    <span class="nc">Consumed</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">stringSerde</span><span class="o">));</span>
<span class="nc">KStream</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">String</span><span class="o">&gt;</span> <span class="n">rightStream</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="na">stream</span><span class="o">(</span><span class="s">"right-topic"</span><span class="o">,</span>
    <span class="nc">Consumed</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">stringSerde</span><span class="o">));</span>
<span class="n">leftStream</span>
    <span class="o">.</span><span class="na">join</span><span class="o">(</span><span class="n">rightStream</span><span class="o">,</span>
        <span class="o">(</span><span class="n">leftValue</span><span class="o">,</span> <span class="n">rightValue</span><span class="o">)</span> <span class="o">-&gt;</span> <span class="s">"["</span> <span class="o">+</span> <span class="n">leftValue</span> <span class="o">+</span> <span class="s">", "</span> <span class="o">+</span> <span class="n">rightValue</span> <span class="o">+</span> <span class="s">"]"</span><span class="o">,</span>
        <span class="nc">JoinWindows</span><span class="o">.</span><span class="na">of</span><span class="o">(</span><span class="nc">Duration</span><span class="o">.</span><span class="na">ofMinutes</span><span class="o">(</span><span class="mi">5</span><span class="o">)),</span>
        <span class="nc">StreamJoined</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">stringSerde</span><span class="o">,</span> <span class="n">stringSerde</span><span class="o">)</span>
    <span class="o">)</span>
    <span class="o">.</span><span class="na">to</span><span class="o">(</span><span class="s">"joined-topic"</span><span class="o">,</span> <span class="nc">Produced</span><span class="o">.</span><span class="na">with</span><span class="o">(</span><span class="n">stringSerde</span><span class="o">,</span> <span class="n">stringSerde</span><span class="o">));</span>

<span class="nc">Topology</span> <span class="n">topology</span> <span class="o">=</span> <span class="n">builder</span><span class="o">.</span><span class="na">build</span><span class="o">();</span>
<span class="k">return</span> <span class="n">topology</span><span class="o">;</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>What does the above code do? Discuss with your peers.</p>
</div>
</li>
<li>
<p>Note the file <code>log4j.properties</code> in the folder <code>src/main/resources</code>, which is used to configure logging for our streams application.</p>
</li>
<li>
<p>Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> to run your code.  Let your Kafka Streams app remain running in the VS Code debugger.</p>
<div class="paragraph">
<p>You should get this output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="k">***</span> Starting Join Sample Application <span class="k">***</span>
...</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Ignore the WARNINGS.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_creating_input_data_2">Creating input Data</h4>
<div class="olist arabic">
<ol class="arabic" start="10">
<li>
<p>Open 3 terminal windows and arrange them side by side so you can see all three of them at the same time.</p>
<div class="paragraph">
<p>We will be using the tool <code>kafkacat</code> to generate data and monitor the output:</p>
</div>
</li>
<li>
<p>In the first terminal window start the tool <code>kafkacat</code> as a producer for the <code>left-topic</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre><strong>kafkacat \
-b kafka:9092 \
-t left-topic \
-P -K: -Z</strong></pre>
</div>
</div>
</li>
<li>
<p>In the second terminal window start <code>kafkacat</code> as a producer for the <code>right-topic</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre><strong>kafkacat \
-b kafka:9092 \
-t right-topic \
-P -K: -Z</strong></pre>
</div>
</div>
</li>
<li>
<p>In the third terminal window run an instance of <code>kafkacat</code> as a consumer of the <code>joined-topic</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre><strong>kafkacat \
-b kafka:9092 \
-t joined-topic \
-C -K\\t</strong></pre>
</div>
</div>
</li>
<li>
<p>In window 1 enter <code>FL:</code> (a record key of <code>FL</code> for Florida a <code>&lt;NULL&gt;</code> value for the record value) and observe the output in window 3. Hint: Nothing should happen. &#8230;&#8203;why?</p>
</li>
<li>
<p>In window 2 also enter the value <code>FL:</code> and observe the output in window 3. Hint: Nothing should happen. &#8230;&#8203;why?</p>
</li>
<li>
<p>Now in window 1 enter the value <code>FL:Orlando</code> and observe the output in window 3. Hint: Nothing should happen. &#8230;&#8203;why?</p>
</li>
<li>
<p>In window 2 enter the value <code>FL:Tampa</code> and observe the output in window 3. You should see:</p>
<div class="literalblock">
<div class="content">
<pre>FL  [Orlando, Tampa]</pre>
</div>
</div>
</li>
<li>
<p>Back in window 1 enter <code>FL:</code> and observe the output in window 3. What do you see? Why?</p>
</li>
<li>
<p>Still in window 1 enter <code>FL:Miami</code> and observe the output in window 3. You should see:</p>
<div class="literalblock">
<div class="content">
<pre>FL  [Miami, Tampa]</pre>
</div>
</div>
</li>
<li>
<p>Continue with window 2 and value <code>FL:Naples</code>. What do you see this time?</p>
</li>
<li>
<p>Continue to enter more values with the same key <code>FL</code>. Here are other cities in Florida to experiment with:</p>
<div class="ulist">
<ul>
<li>
<p>Jacksonville</p>
</li>
<li>
<p>Alachua</p>
</li>
<li>
<p>Pensacola</p>
</li>
<li>
<p>Destin</p>
</li>
<li>
<p>Fort Meyers</p>
</li>
</ul>
</div>
</li>
<li>
<p>What happens if you use a different key, say <code>NY</code>? Why? Discuss the results with your peers.</p>
</li>
<li>
<p>What happens when an event falls outside of the tumbling window?</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_optional_extra_content_for_ksqldb_inner_join">Optional: Extra Content for ksqlDB INNER Join</h4>
<div class="olist arabic">
<ol class="arabic" start="24">
<li>
<p>Go to Control Center <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a> and click on <strong>ksqlDB</strong> at the left pane to open your <code>ksqldb</code> application.</p>
</li>
<li>
<p>In the Editor box, paste the following queries to create the ksqlDB streams with the data from the <code>left-topic</code> and <code>right-topic</code>, and then click <strong>Run query</strong>:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno"> 1
 2
 3
 4
 5
 6
 7
 8
 9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
</pre></td><td class="code"><pre>CREATE STREAM left_stream_kafka (
    left_key STRING KEY,
    left_value STRING)
  WITH (
    KAFKA_TOPIC='left-topic',
    VALUE_FORMAT='kafka');

CREATE STREAM left_stream
  WITH(
    VALUE_FORMAT='avro')
  AS SELECT * FROM left_stream_kafka;

CREATE STREAM right_stream_kafka (
    right_key STRING KEY,
    right_value STRING)
  WITH (
    KAFKA_TOPIC='right-topic',
    VALUE_FORMAT='kafka');

CREATE STREAM right_stream
  WITH(
    VALUE_FORMAT='avro')
  AS SELECT * FROM right_stream_kafka;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
We need to duplicate the streams because <code>KAFKA</code> format does not yet support <code>JOIN</code>. The <code>KAFKA</code> format is primarily intended for use as a key format. It can be used as a value format, but can not be used in any operation that requires a repartition or changelog topic. Removing this limitation requires enhancements to the core of KSQL. This will come in a future release. Until then, avoid using the <code>KAFKA</code> format for values.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Using the documentation about <a href="https://docs.ksqldb.io/en/latest/developer-guide/joins/join-streams-and-tables/#join-clause" target="_blank" rel="noopener">JOIN</a> and <a href="https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/select-push-query/#within-and-grace-period" target="_blank" rel="noopener">WITHIN</a>, try to write the KSQL query to perform the same join operation you have done in the previous section using a Kafka Streams application.</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>SELECT
     left_key AS key,
     '[' + left_value + ', ' + right_value + ']' AS joined_value
  FROM left_stream
    JOIN right_stream
      WITHIN 5 MINUTES
      ON left_key = right_key
  EMIT CHANGES;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</div>
</details>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_left_join">Left Join</h4>
<div class="olist arabic">
<ol class="arabic" start="27">
<li>
<p>Modify  the <code>getTopology()</code> function and replace the <code>join</code> function with a <code>leftJoin</code> function instead.</p>
</li>
<li>
<p>Recompile and run the application.</p>
</li>
<li>
<p>This time, use a car brand for the key and different car models for values to play with input data as you have done for the (inner) join example.</p>
<div class="paragraph">
<p>What is different? Discuss with your peers if needed.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_optional_extra_content_for_ksqldb_left_join">Optional: Extra Content for ksqlDB LEFT Join</h4>
<div class="olist arabic">
<ol class="arabic" start="30">
<li>
<p>Go to Control Center <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a> and click on <strong>ksqlDB</strong> at the left pane to open your <code>ksqldb</code> application.</p>
</li>
<li>
<p>Try to write the KSQL query to perform the left join operation.</p>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>SELECT
     left_key AS key,
     '[' + left_value + ', ' + right_value + ']' AS joined_value
  FROM left_stream
    LEFT JOIN right_stream
      WITHIN 5 MINUTES
      ON left_key = right_key
  EMIT CHANGES;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</div>
</details>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_outer_join">Outer Join</h4>
<div class="olist arabic">
<ol class="arabic" start="32">
<li>
<p>Modify  the <code>getTopology()</code> function and replace the current <strong>join</strong> function with an <code>outerJoin</code> function instead.</p>
</li>
<li>
<p>Recompile and run the application.</p>
</li>
<li>
<p>This time, use your home country for key and different cities for values to play with input data as you have done for the (inner) join example and observe the output.</p>
<div class="paragraph">
<p>What is different? Discuss with your peers if needed.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_optional_extra_content_for_ksqldb_outer_join">Optional: Extra Content for ksqlDB OUTER Join</h4>
<div class="olist arabic">
<ol class="arabic" start="35">
<li>
<p>Go to Control Center <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a> and click on <strong>ksqlDB</strong> at the left pane to open your <code>ksqldb</code> application.</p>
</li>
<li>
<p>Try to write the KSQL query to perform the outer join operation.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
TIP: Check the function <a href="https://docs.ksqldb.io/en/latest/developer-guide/ksqldb-reference/scalar-functions/#ifnull"><code>IFNULL</code></a>, you may need to use it.
</td>
</tr>
</table>
</div>
<details>
<summary class="title">Solution</summary>
<div class="content">
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="ksqlDB"><table class="linenotable"><tbody><tr><td class="linenos gl"><pre class="lineno">1
2
3
4
5
6
7
8
</pre></td><td class="code"><pre>SELECT
     IFNULL(left_key, right_key) AS key,
     '[' + left_value + ', ' + right_value + ']' AS joined_value
  FROM left_stream
    FULL JOIN right_stream
      WITHIN 5 MINUTES
      ON left_key = right_key
  EMIT CHANGES;
</pre></td></tr></tbody></table></code></pre>
</div>
</div>
</div>
</details>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_optional_stream_table_joins">Optional: Stream - Table Joins</h4>
<div class="olist arabic">
<ol class="arabic" start="37">
<li>
<p>Perform a similar experiment with a stream - table <strong>left join</strong> (the most common join in most streaming applications). Make sure to experiment with sending null keys and values. How are the results different from the stream - stream left join?</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_9">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="38">
<li>
<p>Terminate the first 2 instances of <code>kafkacat</code> (the producer instances) by pressing <code>Ctrl+D</code>.</p>
</li>
<li>
<p>Terminate the third instance of <code>kafkacat</code> (the consumer instance) by pressing <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Terminate your <strong>Kafka Streams</strong> application with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_10">Conclusion</h4>
<div class="paragraph">
<p>In this exercise you created a <strong>Kafka Streams</strong> application and <strong>KSQL queries</strong> that join two streams with inner, left, and outer joins. You then created data for the left and the right input stream and observed the generated output. You used the command line tool <code>kafkacat</code> to generate input data and observe output data. Consider summarizing your observations and comparing them to the information found here: <a href="https://docs.confluent.io/current/streams/developer-guide/dsl-api.html#kstream-kstream-join">https://docs.confluent.io/current/streams/developer-guide/dsl-api.html#kstream-kstream-join</a>. Especially focus on the subsection called "Semantics of stream-stream joins" with an illustrative table of the output records that are produced from a join as events flow into the left and right streams.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_08_custom_processing">Lab 08 Custom Processing</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_a_using_the_processor_api">a. Using the Processor API</h3>
<div class="paragraph">
<p>The purpose of this exercise is to create an application with the lower-level Processor API. This may be required in applications that require a greater level of control over state store management and more sophisticated application logic than the Kafka Streams DSL can provide.</p>
</div>
<div class="paragraph">
<p>This exercise will give you experience creating a Kafka Streams application using the DSL and creating a new node in the topology using the Processor API through the transform() method.</p>
</div>
<div class="paragraph">
<p>This application uses a simple source &#8594; word count processor &#8594; sink processing topology. The source node takes in records from an input topic whose values are sentence strings. The word count processor uses each record&#8217;s value to update its internal state store for word counts (key=word, value=count) and sends that state to the sink processor every second. The sink processor produces the resulting records to an output topic.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_10">Prerequisites</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Please make sure you have prepared your lab environment as described here: &#8594;<a href="#preparing-lab">Lab Environment</a></p>
</li>
<li>
<p>If your Kafka cluster is not running already, start it with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/processor-api</strong>
$ <strong>docker-compose up -d zookeeper kafka</strong></pre>
</div>
</div>
</li>
<li>
<p>Create two topics called <code>lines-topic</code>, and <code>word-count-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic lines-topic</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic word-count-topic</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_create_the_streaming_app_2">Create the Streaming App</h4>
<div class="olist arabic">
<ol class="arabic" start="4">
<li>
<p>Open another terminal and navigate to the <code>processor-api</code> folder, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/processor-api</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Open the file <code>build.gradle</code> in the <code>processor-api</code> folder and analyze its content. It should be quite familiar by now.</p>
</li>
<li>
<p>Open the file  <code>WordCountTransformer.java</code> in subfolder <code>src/main/java/streams</code>.</p>
<div class="paragraph">
<p>Familiarize yourself with the code. We are creating an instance of type <code>Transformer</code> and overriding its methods. Thoroughly document what the <code>init</code> method does. Pay particular note to the call to the context.schedule() method, with its punctuation and the call to context.forward(). As always, you can check the corresponding code in the <code>solutions</code> folder for feedback.</p>
</div>
</li>
<li>
<p>You can challenge yourself to write the <code>transform</code> method, or continue with the next step.</p>
</li>
<li>
<p>Make it so the <code>transform</code> method of the class has these contents:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nc">Long</span> <span class="n">oldValue</span> <span class="o">=</span> <span class="k">this</span><span class="o">.</span><span class="na">kvStore</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="n">word</span><span class="o">);</span>
<span class="k">if</span> <span class="o">(</span><span class="n">oldValue</span> <span class="o">==</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
   <span class="k">this</span><span class="o">.</span><span class="na">kvStore</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="mi">1L</span><span class="o">);</span>
<span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
   <span class="k">this</span><span class="o">.</span><span class="na">kvStore</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="n">word</span><span class="o">,</span> <span class="n">oldValue</span> <span class="o">+</span> <span class="mi">1L</span><span class="o">);</span>
<span class="o">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>This code gets the correct entry from the keystore and updates it or it creates a new entry in the keystore with a key of the incoming word and a count of 1. It leaves it to the context scheduler to forward the entries in the key value store, producing them to Kafka for durability.</p>
</div>
</li>
<li>
<p>Open the file <code>CustomTransformerApp.java</code> in the same folder, and familiarize yourself with the code. This code is entirely DSL - It defines the topology, creates the configuration, sets up the shutdown hook and starts the <strong>Kafka Streams</strong> app.</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>In the topology, it creates a stream from the input topic, uses the flatMapValues() method to break up the lines of input text into individual words, writes a rekeyed stream out to a repartition topic and reads it back in. Then it calls the transform() method we created in the WordCountTransformer code.  And finally, it directs those results to the output topic.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="10">
<li>
<p>Notice the file <code>log4j.properties</code> in the folder <code>src/main/resources</code> used to configure logging for the application.</p>
</li>
<li>
<p>Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> to run your code.  Let your Kafka Streams app remain running in the VS Code debugger.</p>
<div class="paragraph">
<p>You should get this output:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash"><span class="k">***</span> Starting Custom Transformer App Application <span class="k">***</span></code></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_creating_input_data_3">Creating input Data</h4>
<div class="olist arabic">
<ol class="arabic" start="12">
<li>
<p>Open 2 terminal windows and arrange them side by side so you can see the two at the same time.</p>
</li>
<li>
<p>In the first terminal window start the tool <code>kafkacat</code> as a producer for the <code>lines-topic</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre><strong>kafkacat \
-b kafka:9092 \
-t lines-topic \
-P -K :</strong></pre>
</div>
</div>
</li>
<li>
<p>In the second terminal window run an instance of <code>kafka-console-consumer</code> as a consumer of the <code>word-count-topic</code> topic, printing the String key and the Long value:</p>
<div class="listingblock">
<div class="content">
<pre><strong>kafka-console-consumer --bootstrap-server kafka:9092 \
 --topic word-count-topic --from-beginning \
 --property print.key=true \
 --value-deserializer org.apache.kafka.common.serialization.LongDeserializer</strong></pre>
</div>
</div>
</li>
<li>
<p>In window 1 enter each of these strings one at a time:</p>
<div class="listingblock">
<div class="content">
<pre><strong>kafka:Kafka powers the Confluent streaming platform
kafka:A streaming application uses Kafka
kafka:Everyone loves Kafka
kafka:Many contributors to Kafka work for Confluent</strong></pre>
</div>
</div>
<div class="paragraph">
<p>After each line observe the output in window 2.</p>
</div>
</li>
<li>
<p>Discuss the result with your peers.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_10">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="17">
<li>
<p>Terminate the <code>kafkacat</code> instance by pressing <code>Ctrl+D</code>.</p>
</li>
<li>
<p>Terminate the <code>kafka-console-consumer</code> instance by pressing <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Terminate your <strong>Kafka Streams</strong> application with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_11">Conclusion</h4>
<div class="paragraph">
<p>In this sample we have demonstrated the use of a custom transform written using the Processor API to count words. This is included into a Stream processing application written entirely in the Streams DSL that takes in sentences from an input topic, process them and writes the resulting word and count pairs to an output topic.</p>
</div>
<div class="paragraph">
<p> <br>
 <br></p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_09_testing_monitoring_and_troubleshooting">Lab 09 Testing, Monitoring and Troubleshooting</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This lab contains 5 exercises:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Testing:</p>
<div class="ulist">
<ul>
<li>
<p>Building Unit Tests</p>
</li>
<li>
<p>Integration Tests using Embedded Kafka</p>
</li>
</ul>
</div>
</li>
<li>
<p>Monitoring:</p>
<div class="ulist">
<ul>
<li>
<p>Getting Metrics from a Kafka Streams Application</p>
</li>
<li>
<p>Using JConsole to monitor a Streams App</p>
</li>
<li>
<p>Monitoring a Kafka Streams App in Confluent Control
Center</p>
</li>
</ul>
</div>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_a_building_unit_tests">a. Building Unit Tests</h3>
<div class="paragraph">
<p>It is essential for every application to have full test coverage of each of its components. The purpose of this exercise is to build unit tests for an existing Kafka Streams application and test the application with Gradle.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_11">Prerequisites</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</li>
<li>
<p>Notice that we do not need to run our Kafka cluster - the unit and integration testing do not require it.</p>
</li>
<li>
<p>Navigate to the folder <code>~/confluent-streams/labs/testing/simple-test</code>, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/testing/simple-test</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Open the file <code>build.gradle</code> inside this folder and analyze its content. Notice the <code>junit</code>, as well as the <code>kafka-streams-test-utils</code> dependencies that we use to enable testing.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_authoring_the_processor">Authoring the Processor</h4>
<div class="olist arabic">
<ol class="arabic" start="5">
<li>
<p>Have a look at the content of subfolder <code>src/main/java/streams</code>. You should find 3 Java files in it:</p>
<div class="ulist">
<ul>
<li>
<p><code>CustomMaxAggregatorSupplier.java</code></p>
</li>
<li>
<p><code>ConfigProvider.java</code></p>
</li>
<li>
<p><code>TopologyProvider.java</code></p>
</li>
</ul>
</div>
</li>
<li>
<p>Open the class <code>CustomMaxAggregatorSupplier</code> and have a look into the code and try to understand what&#8217;s happening. This is the code we&#8217;re going to test ultimately. Maybe discuss the code with your peers.</p>
</li>
<li>
<p>Now a <strong>Kafka Streams</strong> application also needs some configuration. For this purpose we have the <code>ConfigProvider</code> class. Once again make sure you understand the code before you proceed.</p>
</li>
<li>
<p>Finally we have a <code>TopologyProvider</code> class which defines the topology of the <strong>Kafka Streams</strong> app that we want to test. And again we invite you to analyze the code and discuss it with your peers if needed.</p>
</li>
<li>
<p>Due to the fact that we use this topology in a test scenario it has some settings that would not be recommended in production. Can you spot them? If yes, discuss how we could improve this class to work well for both scenarios, testing and production run.</p>
</li>
<li>
<p>Now we&#8217;re ready for the actual test class. Open the class <code>ProcessorTest.java</code> located in subfolder <code>src/test/java/streams</code>. This file contains the skeleton of the test class.</p>
<div class="paragraph">
<p>Let&#8217;s discuss the code:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>We are using the <code>TopologyTestDriver</code> class to test the topology. This is a helper class from the <code>kafka-streams-test-utils</code> library.</p>
</li>
<li>
<p>To send records to the test driver we are using the <code>TestInputTopic</code>  class.</p>
</li>
<li>
<p>To verify the results of our application we are using the <code>TestOutputTopic</code> class.</p>
</li>
<li>
<p>In the <code>setup()</code> method, we&#8217;re using our two classes <code>ConfigProvider</code> and <code>TopologyProvider</code> to get the configuration and topology of our <strong>Kafka Streams</strong> application.</p>
</li>
<li>
<p>With the latter two we create a test driver instance that we will use in our tests</p>
</li>
<li>
<p>The processor is stateful and we pre-populate the state store with a value of <code>21</code> for the key <code>a</code>.</p>
</li>
<li>
<p>In the tear down method we simply clean up by closing the test driver instance.</p>
</li>
</ul>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_writing_a_test">Writing a Test</h4>
<div class="paragraph">
<p>Now we are ready to write our first test.</p>
</div>
<div class="olist none">
<ol class="none">
<li>
<p>You may want to examine the Javadocs for the following classes we will be using:</p>
</li>
<li>
<p><strong>TopologyTestDriver:</strong>
<a href="https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/TopologyTestDriver.html" target="_blank" rel="noopener">https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/TopologyTestDriver.html</a></p>
</li>
<li>
<p><strong>TestInputTopic:</strong>
<a href="https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/TestInputTopic.html" target="_blank" rel="noopener">https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/TestInputTopic.html</a></p>
</li>
<li>
<p><strong>TestOutputTopic:</strong>
<a href="https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/TestOutputTopic.html" target="_blank" rel="noopener">https://kafka.apache.org/25/javadoc/org/apache/kafka/streams/TestOutputTopic.html</a></p>
</li>
</ol>
</div>
<div class="olist arabic">
<ol class="arabic" start="11">
<li>
<p>The first test method to the <code>ProcessorTest</code> class will test whether reading from the <code>result-topic</code> will give the current max value of <code>21</code> for the key <code>a</code> after inputting a smaller value for the same key. If so, this assures us that the first input flushed to the local state store and the result was produced to the output topic. You can choose to implement this test yourself by researching the <code>TopologyTestDriver</code>, <code>TestInputTopic</code>, and <code>TestOutputTopic</code> classes as well as the org.hamcrest.MatcherAssert and org.hamcrest.CoreMatchers classes before moving on.</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nd">@Test</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">shouldFlushStoreForFirstInput</span><span class="o">()</span> <span class="o">{</span>
    <span class="c1">// TODO: add test code here</span>
<span class="o">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>So far this is just standard <code>jUnit</code> test method.</p>
</div>
</li>
<li>
<p>Add code inside the above method to create an input record using the <code>pipeInput</code> method of the <code>TestInputTopic</code> class:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java">    <span class="n">inputTopic</span><span class="o">.</span><span class="na">pipeInput</span><span class="o">(</span><span class="s">"a"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">);</span></code></pre>
</div>
</div>
</li>
<li>
<p>We can use the <code>readKeyValue</code> method of the <code>TestOutputTopic</code> class to read the output record generated by the processor and the <code>assertThat</code> method to compare the key value with the expected result</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="n">assertThat</span><span class="o">(</span><span class="n">outputTopic</span><span class="o">.</span><span class="na">readKeyValue</span><span class="o">(),</span> <span class="n">equalTo</span><span class="o">(</span><span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"a"</span><span class="o">,</span> <span class="mi">21L</span><span class="o">)));</span></code></pre>
</div>
</div>
</li>
<li>
<p>Finally we make sure that this was the only result that we got as output for the given input:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="n">assertThat</span><span class="o">(</span><span class="n">outputTopic</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">(),</span> <span class="n">is</span><span class="o">(</span><span class="kc">true</span><span class="o">));</span></code></pre>
</div>
</div>
</li>
<li>
<p>The final test method should look like this:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nd">@Test</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">shouldFlushStoreForFirstInput</span><span class="o">()</span> <span class="o">{</span>
    <span class="c1">// TODO: add test code here</span>
    <span class="n">inputTopic</span><span class="o">.</span><span class="na">pipeInput</span><span class="o">(</span><span class="s">"a"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">);</span>
    <span class="n">assertThat</span><span class="o">(</span><span class="n">outputTopic</span><span class="o">.</span><span class="na">readKeyValue</span><span class="o">(),</span> <span class="n">equalTo</span><span class="o">(</span><span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"a"</span><span class="o">,</span> <span class="mi">21L</span><span class="o">)));</span>
    <span class="n">assertThat</span><span class="o">(</span><span class="n">outputTopic</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">(),</span> <span class="n">is</span><span class="o">(</span><span class="kc">true</span><span class="o">));</span>
<span class="o">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>And that&#8217;s it! We have just authored a complete test.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_tests_displaying_the_test_report">Running the Test(s) &amp;  Displaying the Test Report</h4>
<div class="olist arabic">
<ol class="arabic" start="16">
<li>
<p>In the terminal window:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>./gradlew test</strong></pre>
</div>
</div>
<div class="paragraph">
<p>The output of this command should look similar to this:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash">BUILD SUCCESSFUL <span class="k">in </span>4s
3 actionable tasks: 2 executed, 1 up-to-date</code></pre>
</div>
</div>
<div class="paragraph">
<p>As we can see, the code compiled and the tests ran without a problem.</p>
</div>
</li>
<li>
<p>Navigate to the <code>simple-test/build/reports/tests/test/</code> folder and open the <code>index.html</code> test report in a browser.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Open from file explorer providing the full path <code>confluent-streams/labs/testing/simple-test/build/reports/tests/test/</code>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>it should look similar to this:</p>
</div>
<div class="imageblock">
<div class="content">
<img src="./images/testing-monitoring-troubleshooting/test-report.png" alt="test report">
</div>
</div>
<div class="paragraph">
<p>At this point, we should see 1 test that ran successfully.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_adding_more_tests">Adding more Tests</h4>
<div class="olist arabic">
<ol class="arabic" start="18">
<li>
<p>Provided is a second test that makes sure that the store value is not updated for smaller input values</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nd">@Test</span>
<span class="kd">public</span> <span class="kt">void</span> <span class="nf">shouldNotUpdateStoreForSmallerValue</span><span class="o">()</span> <span class="o">{</span>
    <span class="n">inputTopic</span><span class="o">.</span><span class="na">pipeInput</span><span class="o">(</span><span class="s">"a"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">);</span>
    <span class="n">assertThat</span><span class="o">(</span><span class="n">store</span><span class="o">.</span><span class="na">get</span><span class="o">(</span><span class="s">"a"</span><span class="o">),</span> <span class="n">equalTo</span><span class="o">(</span><span class="mi">21L</span><span class="o">));</span>
    <span class="n">assertThat</span><span class="o">(</span><span class="n">outputTopic</span><span class="o">.</span><span class="na">readKeyValue</span><span class="o">(),</span> <span class="n">equalTo</span><span class="o">(</span><span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"a"</span><span class="o">,</span> <span class="mi">21L</span><span class="o">)));</span>
    <span class="n">assertThat</span><span class="o">(</span><span class="n">outputTopic</span><span class="o">.</span><span class="na">isEmpty</span><span class="o">(),</span> <span class="n">is</span><span class="o">(</span><span class="kc">true</span><span class="o">));</span>
<span class="o">}</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>Discuss with your peers what this test does.</p>
</div>
</li>
<li>
<p>Add more tests and repeat the <code>./gradlew test</code> command to see new results.</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Add a test that asserts that the store value is updated for a larger input value.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
If you get stuck, please have a look in the solutions folder where you will find the complete sample solution.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Add a test that asserts a new store value is generated if adding a value with a new key "b". Also make sure the existing store value for "a" is unchanged.</p>
</li>
<li>
<p>Write a test that verifies that the processor <strong>punctuates</strong> if the <strong>event time</strong> advances.</p>
</li>
<li>
<p>Write a test that verifies that the processor <strong>punctuates</strong> if the <strong>wall clock time</strong> advances.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Use the function <code>testDriver.advanceWallClockTime(&#8230;&#8203;)</code> for this.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_12">Conclusion</h4>
<div class="paragraph">
<p>In this sample we have shown how to test a simple <strong>Kafka Streams</strong> application that uses the Processor API.</p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div class="sect2">
<h3 id="_b_integration_tests_using_embedded_kafka">b. Integration Tests using Embedded Kafka</h3>
<div class="paragraph">
<p>In this exercise, we are going a step further to use a full blown <strong>embedded</strong> Kafka single node cluster to test our <strong>Kafka Streams</strong> application. The application is a simple <strong>word count</strong> streaming app. As in the previous test, we do not need to run our Docker container Kafka cluster.</p>
</div>
<div class="sect3">
<h4 id="_prerequistes">Prerequistes</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_creating_the_artifacts_to_test">Creating the Artifacts to Test</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to folder <code>~/confluent-streams/labs/testing/wordcount-test</code>, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/testing/wordcount-test</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Open the file <code>build.gradle</code> and analyze its content. Notice the many dependencies that we have to add to enable this kind of integration testing.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
we have a few dependencies that use the <strong>test</strong> version of the respective <code>JAR</code> files. This can be achieved by adding <code>classifier: 'test'</code> or <code>classifier: 'tests'</code> to the dependency.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Open the file <code>TopologyProvider.java</code> class, located in subfolder <code>src/main/java/streams</code>, that defines the <strong>Kafka Streams</strong> topology that we will test. Analyze the code. Make sure you understand it. If not discuss it with your peers.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_writing_the_integration_test">Writing the Integration Test</h4>
<div class="paragraph">
<p>Now it is time to write the actual test that is leveraging the testbed that we have just prepared.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="4">
<li>
<p>Locate the file <code>TopologyProviderTest.java</code> in subfolder <code>src/test/java/streams</code> and open it. It will host our test code.</p>
<div class="paragraph">
<p>Let&#8217;s analyze the code a bit:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>In the <code>@BeforeClass</code> we&#8217;re initializing and running an embedded single node Kafka cluster. We are preparing the cluster by creating the <strong>input</strong> and the <strong>output</strong> topic.</p>
</li>
<li>
<p>The method <code>shouldCountWords</code> contains our first test. We have clearly documented the steps:</p>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>define the configuration of the streaming app to use during the test</p>
</li>
<li>
<p>get the topology that we want to test</p>
</li>
<li>
<p>initialize and start the streaming application</p>
</li>
<li>
<p>produce some data for the <strong>input</strong> topic</p>
</li>
<li>
<p>finally verify that the produced data in the <strong>output</strong> topic corresponds to the expected values</p>
</li>
</ol>
</div>
</li>
</ul>
</div>
</li>
<li>
<p>Inspect the method <code>getStreamsConfiguration</code>. Notice that we take the information about the bootstrap server(s) from our <code>CLUSTER</code> variable which represents our single node embedded Kafka cluster. We also provide a path to where the state store should store its information (<code>STATE_DIR_CONFIG</code>).</p>
</li>
<li>
<p>Inspect the method <code>produceInputData</code>. Notice that we&#8217;re configuring a producer and are using it to feed the data into the <strong>input</strong> topic.</p>
</li>
<li>
<p>The final step is to write code that validates the results. Inspect the <code>verifyOutputData</code> method. You can challenge yourself to complete the method, or you can proceed to the next step.</p>
</li>
<li>
<p>Make it so the <code>verifyOutputData</code> method has these contents:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="nc">List</span><span class="o">&lt;</span><span class="nc">KeyValue</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;&gt;</span> <span class="n">expectedWordCounts</span> <span class="o">=</span> <span class="nc">Arrays</span><span class="o">.</span><span class="na">asList</span><span class="o">(</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"hello"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"all"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"streams"</span><span class="o">,</span> <span class="mi">2L</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"lead"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"to"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"join"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"kafka"</span><span class="o">,</span> <span class="mi">3L</span><span class="o">),</span>
    <span class="k">new</span> <span class="nc">KeyValue</span><span class="o">&lt;&gt;(</span><span class="s">"summit"</span><span class="o">,</span> <span class="mi">1L</span><span class="o">)</span>
<span class="o">);</span>

<span class="nc">Properties</span> <span class="n">consumerConfig</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">Properties</span><span class="o">();</span>
<span class="n">consumerConfig</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">ConsumerConfig</span><span class="o">.</span><span class="na">BOOTSTRAP_SERVERS_CONFIG</span><span class="o">,</span> <span class="no">CLUSTER</span><span class="o">.</span><span class="na">bootstrapServers</span><span class="o">());</span>
<span class="n">consumerConfig</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">ConsumerConfig</span><span class="o">.</span><span class="na">GROUP_ID_CONFIG</span><span class="o">,</span>
    <span class="s">"wordcount-lambda-integration-test-standard-consumer"</span><span class="o">);</span>
<span class="n">consumerConfig</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">ConsumerConfig</span><span class="o">.</span><span class="na">AUTO_OFFSET_RESET_CONFIG</span><span class="o">,</span> <span class="s">"earliest"</span><span class="o">);</span>
<span class="n">consumerConfig</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">ConsumerConfig</span><span class="o">.</span><span class="na">KEY_DESERIALIZER_CLASS_CONFIG</span><span class="o">,</span>
    <span class="nc">StringDeserializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>
<span class="n">consumerConfig</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">ConsumerConfig</span><span class="o">.</span><span class="na">VALUE_DESERIALIZER_CLASS_CONFIG</span><span class="o">,</span>
    <span class="nc">LongDeserializer</span><span class="o">.</span><span class="na">class</span><span class="o">);</span>

<span class="nc">List</span><span class="o">&lt;</span><span class="nc">KeyValue</span><span class="o">&lt;</span><span class="nc">String</span><span class="o">,</span> <span class="nc">Long</span><span class="o">&gt;&gt;</span> <span class="n">actualWordCounts</span> <span class="o">=</span>
    <span class="nc">IntegrationTestUtils</span><span class="o">.</span><span class="na">waitUntilMinKeyValueRecordsReceived</span><span class="o">(</span>
        <span class="n">consumerConfig</span><span class="o">,</span> <span class="n">outputTopic</span><span class="o">,</span> <span class="n">expectedWordCounts</span><span class="o">.</span><span class="na">size</span><span class="o">());</span>
<span class="n">assertThat</span><span class="o">(</span><span class="n">actualWordCounts</span><span class="o">,</span> <span class="n">containsInAnyOrder</span><span class="o">(</span><span class="n">expectedWordCounts</span><span class="o">.</span><span class="na">toArray</span><span class="o">()));</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>We define an array with the expected data, define a consumer configuration, and then compare the produced data to the expected data.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_tests">Running the Test(s)</h4>
<div class="olist arabic">
<ol class="arabic" start="9">
<li>
<p>In the terminal window:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>./gradlew test</strong></pre>
</div>
</div>
<div class="paragraph">
<p>You should see something like this (shortened):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="bash">...
<span class="o">&gt;</span> Task :compileJava
<span class="o">&gt;</span> Task :processResources NO-SOURCE
<span class="o">&gt;</span> Task :classes
<span class="o">&gt;</span> Task :compileTestJava
<span class="o">&gt;</span> Task :processTestResources NO-SOURCE
<span class="o">&gt;</span> Task :testClasses
<span class="o">&gt;</span> Task :test

BUILD SUCCESSFUL <span class="k">in </span>46s
3 actionable tasks: 3 executed
...</code></pre>
</div>
</div>
</li>
<li>
<p>Navigate to the <code>wordcount-test/build/reports/tests/test/</code> folder and open the <code>index.html</code> test report in a browser.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_11">Cleanup</h4>
<div class="paragraph">
<p>There is no special cleanup needed.</p>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_13">Conclusion</h4>
<div class="paragraph">
<p>In this exercise we have written an integration test for a Kafka Streams app topology. The test bed uses an embedded version of Kafka and Zookeeper.</p>
</div>
<div class="paragraph">
<p> 
 
 </p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_c_getting_metrics_from_a_kafka_streams_application">c. Getting Metrics from a Kafka Streams Application</h3>
<div class="paragraph">
<p>The purpose of this exercise is to learn how to expose metrics in a Kafka Streams application. This particular application sends metrics to standard output every 10 seconds. In practice, these metrics can be exposed to external sources for aggregation and analysis, as we will see in the exercises that follow.</p>
</div>
<div class="sect3">
<h4 id="_preparing_the_application">Preparing the application</h4>
<div class="paragraph">
<p>In this exercise we&#8217;re going to use the <strong>word count</strong> exercise from a previous lab.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</li>
<li>
<p>Navigate to the folder for this lab:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/monitoring</strong></pre>
</div>
</div>
</li>
<li>
<p>Run the Kafka cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka control-center</strong></pre>
</div>
</div>
</li>
<li>
<p>Create the two topics called <code>lines-topic</code> and <code>word-count-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic lines-topic</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic word-count-topic</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_building_running_the_application">Building &amp; Running the Application</h4>
<div class="olist arabic">
<ol class="arabic" start="5">
<li>
<p>In a terminal window navigate to the <code>word-count</code> folder, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/monitoring/word-count</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Open the file <code>build.gradle</code> in folder <code>word-count</code> and analyze its content. It should be quite familiar by now.</p>
</li>
<li>
<p>Notice the four Java files in subfolder <code>src/main/java/streams</code>:</p>
<table class="tableblock frame-all grid-all stripes-even stretch">
<colgroup>
<col style="width: 30%;">
<col style="width: 70%;">
</colgroup>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>ConfigProvider.java</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Defines the configuration for the streams application</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>MetricsReporter.java</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Defines how the list of metrics is output every 10 seconds by the app</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>TopologyProvider.java</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Defines the topology of the stream application</p>
</div></div></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p><code>WordCountSample.java</code></p>
</div></div></td>
<td class="tableblock halign-left valign-top"><div class="content"><div class="paragraph">
<p>Main class of the application. This class makes use of the 3 following classes</p>
</div></div></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Analyze their code. Specifically note the use of the <code>MetricsReporter</code> class. Make sure you understand what&#8217;s going on in the code.</p>
</div>
</li>
<li>
<p>Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> in VS Code or <code>./gradlew run</code> in the terminal to run your code.</p>
<div class="paragraph">
<p>The application will print out the list of metrics to the terminal every 10 seconds. It should look similar to this (shortened for readability):</p>
</div>
<div class="listingblock">
<div class="content">
<pre>--- Application Metrics ---
MetricName [name=count, group=kafka-metrics-count, description=total number of registered metrics, tags={client-id=wordCount-3e02d8d9-490b-492a-9bf0-cf85629e7fcf-StreamThread-1-producer}], 81.0
MetricName [name=io-time-ns-avg, group=producer-metrics, description=The average length of time for I/O per select call in nanoseconds., tags={client-id=wordCount-3e02d8d9-490b-492a-9bf0-cf85629e7fcf-StreamThread-1-producer}], 612250.0</pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_producing_input_data">Producing Input Data</h4>
<div class="paragraph">
<p>To see how the metrics change, we can produce some input data that the application will process. We use the <code>kafka-console-producer</code> tool for this job.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="9">
<li>
<p>Return to the terminal window and create a list of input sentences that will be randomly produced to the input topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>INPUTS=('Kafka powers the Confluent streaming platform' \
     'All Streams come from Kafka'\
     'Streams will all flow to Kafka'\
     'Follow the streams to Kafka Summit' \
     'Check out Confluent Cloud')</strong></pre>
</div>
</div>
</li>
<li>
<p>Run Kafkacat to produce a steady flow of sentences to the input topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>while true; do
      MESSAGE=${INPUTS[${RANDOM} % ${#INPUTS[@]}]}
      echo ${MESSAGE} | kafkacat -P \
          -b kafka:9092 \
          -t lines-topic
      sleep 0.1
  done</strong></pre>
</div>
</div>
</li>
<li>
<p>Observe the metric values printed by the Word Count application.</p>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Use <code>grep</code> to track a metric. For example, run: <code>./gradlew run | grep name=poll-records-avg</code> to track the metric <strong>poll-records-avg</strong> and then start/stop the kafkacat producer of Step 10.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>In another terminal window run <code>kafka-console-consumer</code> to report the output of our sample app:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-console-consumer --bootstrap-server kafka:9092 \
    --topic word-count-topic \
    --property print.key=true \
    --value-deserializer org.apache.kafka.common.serialization.LongDeserializer</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_12">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="13">
<li>
<p>Quit the Producer with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Quit the Consumer with <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Quit our sample app with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div class="sect2">
<h3 id="_d_using_jconsole_to_monitor_a_streams_app">d. Using JConsole to monitor a Streams App</h3>
<div class="paragraph">
<p>In this exercise, we will use <code>JConsole</code> to monitor the various metrics a simple <strong>Kafka Streams</strong> application exposes.</p>
</div>
<div class="paragraph">
<p>We will be using an application we created in an earlier lab that reads data from a topic whose keys are integers and whose values are sentence strings. The input values are transformed to lower-case and output to a new topic.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_12">Prerequisites</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
</div>
<div class="sect3">
<h4 id="_preparing_the_lab">Preparing the Lab</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Navigate to the module folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/monitoring</strong></pre>
</div>
</div>
</li>
<li>
<p>Run the Kafka cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka</strong></pre>
</div>
</div>
</li>
<li>
<p>Create an input topic called <code>lines-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic lines-topic</strong></pre>
</div>
</div>
</li>
<li>
<p>Create the output topic called <code>lines-lower-topic</code> in Kafka:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic lines-lower-topic</strong></pre>
</div>
</div>
</li>
<li>
<p>In a terminal window navigate to the project folder <code>jmx-sample</code> and build the artifact:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/monitoring/jmx-sample</strong>
$ <strong>./gradlew build</strong></pre>
</div>
</div>
</li>
<li>
<p>Examine <code>build.gradle</code> and observe it has been configured to expose JMX metrics on port <code>4444</code> that we can then use to attach <code>JConsole</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cat build.gradle</strong>
...
applicationDefaultJvmArgs = [
"-Dcom.sun.management.jmxremote",
"-Dcom.sun.management.jmxremote.authenticate=false",
"-Dcom.sun.management.jmxremote.ssl=false",
"-Djava.rmi.server.hostname=127.0.0.1",
"-Dcom.sun.management.jmxremote.rmi.port=4444",
"-Dcom.sun.management.jmxremote.port=4444"]
...</pre>
</div>
</div>
</li>
<li>
<p>Still in the project folder <code>jmx-sample</code> run your <strong>Kafka Streams</strong> application:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>./gradlew run</strong></pre>
</div>
</div>
</li>
<li>
<p>Observe the JMX metrics of the <strong>Kafka Streams</strong> application:</p>
<div class="olist loweralpha">
<ol class="loweralpha" type="a">
<li>
<p>Open a new Terminal window and open a <code>jconsole</code> connection to port <code>4444</code> which is the JMX port for the <strong>Kafka Streams</strong> application.</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>jconsole localhost:4444 &amp;</strong></pre>
</div>
</div>
</li>
<li>
<p>Select <strong>Insecure connection</strong> when asked</p>
<div class="imageblock">
<div class="content">
<img src="./images/testing-monitoring-troubleshooting/jconsole-insecure-connection.png" alt="jconsole insecure connection">
</div>
</div>
</li>
<li>
<p>Navigate to the <strong>MBeans</strong> tab</p>
</li>
<li>
<p>Explore the node under <strong>kafka-streams</strong> as indicated in the below image:</p>
<div class="imageblock">
<div class="content">
<img src="./images/testing-monitoring-troubleshooting/jconsole-metrics-1.png" alt="jconsole metrics 1">
</div>
</div>
<div class="paragraph">
<p>Initially some of the numbers, such as <code>process total</code> will be zero.</p>
</div>
</li>
</ol>
</div>
</li>
<li>
<p>Open a new terminal window and navigate to the <code>monitoring</code> folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/monitoring</strong></pre>
</div>
</div>
</li>
<li>
<p>Now create some data that will be consumed and processed by the <strong>Kafka Streams</strong> application:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cat &lt;&lt; EOF | kafka-console-producer \
    --bootstrap-server kafka:9092 \
    --property "parse.key=true" \
    --property "key.separator=:" \
    --topic lines-topic
1:"Kafka powers the Confluent Streaming Platform"
2:"Events are stored in Kafka"
3:"Confluent contributes to Kafka"
EOF</strong></pre>
</div>
</div>
</li>
<li>
<p>Observe how the values of the metrics in <strong>JConsole</strong> change (you will need to click the <strong>Refresh</strong> button):</p>
<div class="imageblock">
<div class="content">
<img src="./images/testing-monitoring-troubleshooting/jconsole-metrics-2.png" alt="jconsole metrics 2">
</div>
</div>
</li>
<li>
<p><strong>Optional:</strong> add more data to the topic and monitor the attributes and how the values change.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_13">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="13">
<li>
<p>Quit <strong>JConsole</strong>.</p>
</li>
<li>
<p>Stop the sample Kafka Streams application by pressing <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Shut down your Kafka cluster with the <code>docker-compose down -v</code> command.</p>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_14">Conclusion</h4>
<div class="paragraph">
<p>We have used Java code to retrieve the metrics directly from the <code>KafkaStreams</code> object and also configured the <strong>Kafka Streams</strong> sample application to expose JMX data that we then explored using <code>JConsole</code>.</p>
</div>
<div style="page-break-after: always;"></div>
</div>
</div>
<div class="sect2">
<h3 id="_e_monitoring_a_kafka_streams_app_in_confluent_control_center">e. Monitoring a Kafka Streams App in Confluent Control Center</h3>
<div class="paragraph">
<p>In this exercise we&#8217;re going to reuse the <strong>word count</strong> processor API example application from a previous exercise and extend it so that it can be monitored in <strong>Confluent Control Center</strong>.</p>
</div>
<div class="sect3">
<h4 id="_prerequisites_13">Prerequisites</h4>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</li>
<li>
<p>In a terminal window navigate to the <code>processor-sample</code> folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/monitoring/processor-sample</strong></pre>
</div>
</div>
</li>
<li>
<p>Open this application&#8217;s root directory in VS Code.</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>Open the <code>build.gradle</code> file and observe <code>monitoring-interceptors</code> in the list of dependencies:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="gradle"><span class="n">compile</span> <span class="nl">group:</span> <span class="s2">"io.confluent"</span><span class="o">,</span> <span class="nl">name:</span> <span class="s2">"monitoring-interceptors"</span><span class="o">,</span> <span class="nl">version:</span> <span class="s2">"6.0.0"</span></code></pre>
</div>
</div>
<div class="paragraph">
<p>This dependency contains the interceptors classes that we will use to configure our application for monitoring via <strong>Confluent Control Center</strong>.</p>
</div>
</li>
<li>
<p>Open the file <code>ProcessorSample.java</code> (in folder <code>src/main/java/streams</code>) which contains the <code>main</code> function of the sample application. Notice these lines that have been added to the <code>getConfig</code> function:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="n">settings</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">StreamsConfig</span><span class="o">.</span><span class="na">producerPrefix</span><span class="o">(</span><span class="nc">ProducerConfig</span><span class="o">.</span><span class="na">INTERCEPTOR_CLASSES_CONFIG</span><span class="o">),</span>
    <span class="s">"io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"</span><span class="o">);</span>
<span class="n">settings</span><span class="o">.</span><span class="na">put</span><span class="o">(</span><span class="nc">StreamsConfig</span><span class="o">.</span><span class="na">consumerPrefix</span><span class="o">(</span><span class="nc">ConsumerConfig</span><span class="o">.</span><span class="na">INTERCEPTOR_CLASSES_CONFIG</span><span class="o">),</span>
    <span class="s">"io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"</span><span class="o">);</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Please note the <strong>prefix</strong> used for both consumer and producer interceptors.
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>With interceptors configured, we can monitor our app through <strong>Confluent Control Center</strong>.</p>
</div>
</li>
<li>
<p>Start the Kafka cluster and the <strong>Confluent Control Center</strong> server with:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka control-center</strong></pre>
</div>
</div>
<div class="paragraph">
<p>As usual, you are encouraged to inspect the content of the <code>docker-compose</code> file in the ~confluent-streams directory to make sure you understand all the settings and discuss it with your peers. Wait a couple of minutes until the cluster is initialized. Open <strong>Confluent Control Center</strong> at <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a> and wait until it displays the system health.</p>
</div>
</li>
<li>
<p>Create the input and output topics called <code>lines-topic</code> and <code>word-count-topic</code> respectively. For the moment let&#8217;s give them each 1 partition and replication factor 1:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic lines-topic</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9092 \
    --replication-factor 1 \
    --partitions 1 \
    --topic word-count-topic</strong></pre>
</div>
</div>
</li>
<li>
<p>Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> in VS Code or <code>./gradlew run</code> in the terminal to run your streams app.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Ignore the WARNINGS.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_producing_data">Producing Data</h4>
<div class="olist arabic">
<ol class="arabic" start="9">
<li>
<p>Run the <code>kafka-console-producer</code> that we will use to feed some data to the topic <code>lines-topic</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>NS=io.confluent.monitoring.clients.interceptor &amp;&amp; \
kafka-console-producer \
    --bootstrap-server kafka:9092 \
    --topic lines-topic \
    --producer-property \
    "interceptor.classes=${NS}.MonitoringProducerInterceptor"</strong></pre>
</div>
</div>
</li>
<li>
<p>Open another terminal tab and run the <code>kafka-console-consumer</code> for the <code>word-count-topic</code> topic:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>NS=io.confluent.monitoring.clients.interceptor &amp;&amp; \
kafka-console-consumer \
    --group word-count-consumer \
    --bootstrap-server kafka:9092 \
    --topic word-count-topic \
    --property print.key=true \
    --consumer-property \
    interceptor.classes="${NS}.MonitoringConsumerInterceptor"</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Note the use of the group name <code>word-count-consumer</code> and the <code>MonitoringConsumerInterceptor</code> class to enable monitoring of the consumer in Control Center.</p>
</div>
</li>
<li>
<p>In the terminal window where the producer runs, enter a few lines of text such as:</p>
<div class="listingblock">
<div class="content">
<pre><strong>Kafka is powering the Confluent streaming platform
Streaming in real-time is more and more important
Our company will invest in real-time streaming
For this we need to know loads about Kafka and Kafka Streams
ksqlDB is a simpler alternative to Kafka Streams
Everybody loves ksqlDB since no programming is required</strong></pre>
</div>
</div>
<div class="paragraph">
<p>and observe the output in the terminal window where the consumer runs.</p>
</div>
<div class="admonitionblock tip">
<table>
<tr>
<td class="icon">
<i class="fa icon-tip" title="Tip"></i>
</td>
<td class="content">
Add more data at will so that there is some activity ongoing.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>In <strong>Confluent Control Center</strong> (<a href="http://localhost:9021" class="bare">http://localhost:9021</a>) navigate to <strong>Consumers</strong>. You should see something like this:</p>
<div class="imageblock">
<div class="content">
<img src="./images/testing-monitoring-troubleshooting/c3-streams-monitoring.png" alt="c3 streams monitoring">
</div>
</div>
<div class="paragraph">
<p>Click on our Kafka Streams (processor-sample-v0.1.0) application to investigate its metrics.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_14">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="13">
<li>
<p>To stop the producer hit <code>Ctrl+C</code>.</p>
</li>
<li>
<p>To stop the consumer hit <code>Ctrl+C</code>.</p>
</li>
<li>
<p>To stop the <strong>Kafka Streams</strong> application with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>.</p>
</li>
<li>
<p>To stop the Kafka Cluster and delete the volumes execute this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/monitoring/processor-sample</strong>
$ <strong>docker-compose down -v</strong></pre>
</div>
</div>
</li>
</ol>
</div>
<div class="paragraph">
<p> <br>
 <br>
 <br></p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_lab_11_security">Lab 11 Security</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_a_securing_a_kafka_streams_application">a. Securing a Kafka Streams Application</h3>
<div class="paragraph">
<p>In this exercise, we will interact with a secure Kafka cluster in several ways. The cluster is configured to use SASL-PLAIN for authentication and SSL for transport encryption. A certificate creation script is used to create keystores and truststores for clients and brokers so that they can authenticate with each other. There is a client <code>.properties</code> that will be used to create topics, to produce to an input topic, and to consume from an output topic. There is another <code>.properties</code> file to configure the security for a Kafka Streams application. In practice, SSL certificates must be carefully managed, credentials should be created separately for different clients, and permissions should be applied to secure credentials. These areas are beyond the scope of the course. Rather, the purpose of the exercise is to provide hands-on experience with the configurations necessary to connect a Kafka Streams application to a secured cluster.</p>
</div>
<div class="sect3">
<h4 id="_preparing_the_project">Preparing the Project</h4>
<div class="paragraph">
<p>Please make sure you have prepared your lab environment as described here: &#8594; <a href="#preparing-lab">Lab Environment</a></p>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
If you have a cluster running from an earlier lab, shut it down now. This lab depends on the settings in the local docker-compose.yml file. You can check if you have a cluster running with <code>docker-compose ps</code> and you can shutdown a running cluster with <code>docker-compose down -v</code>.
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_preparing_the_certificates">Preparing the Certificates</h4>
<div class="paragraph">
<p>First we need to generate all the necessary certificates and credentials that are used to create a secure (single node) Kafka cluster.</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>In a terminal window, navigate to the folder <code>security/secure-app/scripts/security</code>:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/security/secure-app/scripts/security</strong></pre>
</div>
</div>
</li>
<li>
<p>Take a few minutes to inspect the contents of the files in this directory and discuss them with your peers.</p>
</li>
<li>
<p>Run the <code>certs-create.sh</code> script to generate the necessary certs and credential files:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>./certs-create.sh</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_running_the_cluster">Running the Cluster</h4>
<div class="olist arabic">
<ol class="arabic" start="4">
<li>
<p>In a terminal window navigate to folder <code>security/secure-app</code>, and launch VS Code:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/security/secure-app</strong>
$ <strong>code .</strong></pre>
</div>
</div>
</li>
<li>
<p>From this folder open the file <code>docker-compose.yml</code> and analyze it. Specifically focus on the settings of the security relevant environment variables. Discuss the content with your peers if you don&#8217;t fully understand it.</p>
</li>
<li>
<p>Start the cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose up -d zookeeper kafka control-center</strong></pre>
</div>
</div>
<div class="paragraph">
<p>and wait a couple of minutes until the cluster is initialized. You may observe the progress by using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose ps</strong></pre>
</div>
</div>
<div class="paragraph">
<p>or following the logs, e.g.:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose logs -f kafka</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Press <code>Ctrl+C</code> to stop following the log.</p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_creating_the_application">Creating the Application</h4>
<div class="olist arabic">
<ol class="arabic" start="7">
<li>
<p>In the <code>secure-app</code> folder, locate the file <code>build.gradle</code> and analyze its content. There should be no surprises at this point.</p>
</li>
<li>
<p>Open the file <code>SecureAppSample.java</code> located in the subfolder <code>src/main/java/streams</code> and inspect its contents. As you can see, we&#8217;re using a class <code>ConfigProvider</code> to get us the configuration for the <strong>Kafka Streams</strong> app and a class <code>TopologyProvider</code> to get us the topology for the application. Other than that all is well known and familiar code by now.</p>
</li>
<li>
<p>Now let&#8217;s have a look at the file <code>ConfigProvider.java</code>. Notice that this code gets the configuration settings from a <code>.properties</code> file, which is preferable to hard-coding properties into our application.</p>
</li>
<li>
<p>In the project sub-folder <code>scripts/security</code>, open the file called <code>secureapp-sample.properties</code> analyze its content.</p>
<div class="ulist">
<ul>
<li>
<p>Note that we&#8217;re using <strong>SASL</strong> to authenticate our application with the secured Kafka cluster.</p>
</li>
<li>
<p>The selection of the <strong>protocol</strong> (SASL_SSL) also has an implication on which port we&#8217;re using for the communication with the brokers; <code>9091</code> in this case. The brokers are also listening to <code>SSL</code> on port <code>11091</code>, if we wanted to do mutual SSL instead of SASL + SSL. In that case, we would also have to configure <code>ssl.keystore</code> location and password on the clients so that Brokers could authenticate them.</p>
</li>
<li>
<p>We need to define the <strong>trust store</strong> and its password as well as the login module and the credentials.</p>
</li>
<li>
<p>We&#8217;re also securing the monitoring interceptors so that the app can be monitored from <strong>Confluent Control Center</strong>.</p>
</li>
</ul>
</div>
</li>
<li>
<p>Copy the security files to the location in <code>/etc/kafka/secrets/</code> - remember our user ID is training and the password which will be requested is also training:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>sudo mkdir -p /etc/kafka/secrets/</strong>
$ <strong>sudo cp scripts/security/* /etc/kafka/secrets/</strong></pre>
</div>
</div>
</li>
<li>
<p>Finally have a look at the file <code>TopologyProvider.java</code> which should have this content:</p>
<div class="listingblock">
<div class="content">
<pre class="rouge highlight"><code data-lang="java"><span class="kn">package</span> <span class="nn">streams</span><span class="o">;</span>

<span class="kn">import</span> <span class="nn">org.apache.kafka.streams.Topology</span><span class="o">;</span>
<span class="kn">import</span> <span class="nn">org.apache.kafka.streams.StreamsBuilder</span><span class="o">;</span>

<span class="kd">public</span> <span class="kd">class</span> <span class="nc">TopologyProvider</span> <span class="o">{</span>
    <span class="kd">public</span> <span class="nc">Topology</span> <span class="nf">getTopology</span><span class="o">()</span> <span class="o">{</span>
        <span class="kd">final</span> <span class="nc">StreamsBuilder</span> <span class="n">builder</span> <span class="o">=</span> <span class="k">new</span> <span class="nc">StreamsBuilder</span><span class="o">();</span>
        <span class="n">builder</span><span class="o">.</span><span class="na">stream</span><span class="o">(</span><span class="s">"secure-input"</span><span class="o">).</span><span class="na">to</span><span class="o">(</span><span class="s">"secure-output"</span><span class="o">);</span>
        <span class="k">return</span> <span class="n">builder</span><span class="o">.</span><span class="na">build</span><span class="o">();</span>
    <span class="o">}</span>
<span class="o">}</span></code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Since this sample is all about securing a <strong>Kafka Streams</strong> application we have chosen a very simple <strong>Topology</strong>, it&#8217;s basically a <code>NO-OP</code> topology. We read input from topic <code>secure-input</code> and directly and unmodified output all the messages to the topic <code>secure-output</code>.
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Before we can run our <strong>Kafka Streams</strong> sample application we need to first manually create the topics <code>secure-input</code> and <code>secure-output</code> since we have configured the Kafka cluster to <strong>disable</strong> auto-create of topics (see setting in the <code>docker-compose.yml</code> file). The <code>AdminClient</code> that creates topics must also authenticate with the Kafka cluster, so we must use the <code>--command-config</code> option to point to the <code>.properties</code> file that contains the security configurations.</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9091 \
    --replication-factor 1 \
    --partitions 3 \
    --topic secure-input \
    --command-config /etc/kafka/secrets/client_security.properties</strong>

$ <strong>kafka-topics \
    --create \
    --bootstrap-server kafka:9091 \
    --replication-factor 1 \
    --partitions 3 \
    --topic secure-output \
    --command-config /etc/kafka/secrets/client_security.properties</strong></pre>
</div>
</div>
<div class="admonitionblock warning">
<table>
<tr>
<td class="icon">
<i class="fa icon-warning" title="Warning"></i>
</td>
<td class="content">
<div class="paragraph">
<p>You will see some warnings such as</p>
</div>
<div class="listingblock">
<div class="content">
<pre>2018-10-15 18:12:39 WARN  ProducerConfig:287 - The configuration 'confluent.monitoring.interceptor.security.protocol' was supplied but isn't a known config.</pre>
</div>
</div>
<div class="paragraph">
<p>You can safely ignore these warnings. The properties are necessary monitoring producers and consumers in Confluent Control Center, but are not recognized by the core open source Apache Kafka <code>ProducerConfig</code> and the <code>ConsumerConfig</code> classes.</p>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Now we&#8217;re ready to run the app. Use <strong>Run</strong> &#8594; <strong>Start Debugging</strong> to run your <strong>Kafka Streams</strong> application.</p>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
Ignore the WARNINGS.
</td>
</tr>
</table>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_creating_input_data_4">Creating Input Data</h4>
<div class="paragraph">
<p>To see the sample streams application working we need to generate some data.</p>
</div>
<div class="olist arabic">
<ol class="arabic" start="15">
<li>
<p>In the <code>~/confluent-streams/labs/security/secure-app/scripts/security</code> folder, there is a file called <code>client_security.properties</code> with the following content:</p>
<div class="listingblock">
<div class="content">
<pre>bootstrap.servers=kafka:9091
security.protocol=SASL_SSL
ssl.truststore.location=/etc/kafka/secrets/kafka.client.truststore.jks
ssl.truststore.password=confluent
sasl.mechanism=PLAIN
sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \
    username=\"client\" \
    password=\"client-secret\";

# authenticate the monitor interceptor with Kafka.
confluent.monitoring.interceptor.bootstrap.servers=kafka:9091
confluent.monitoring.interceptor.security.protocol=SASL_SSL
confluent.monitoring.interceptor.ssl.truststore.location=/etc/kafka/secrets/kafka.client.truststore.jks
confluent.monitoring.interceptor.ssl.truststore.password=confluent
confluent.monitoring.interceptor.sasl.mechanism=PLAIN
confluent.monitoring.interceptor.sasl.jaas.config=\
    org.apache.kafka.common.security.plain.PlainLoginModule required \
    username=\"client\" \
    password=\"client-secret\";</pre>
</div>
</div>
<div class="paragraph">
<p>In an earlier step, we copied this file, along with others, to <code>/etc/kafka/secrets/</code>. This file will be used by the client tools that we&#8217;re going to use in a moment to authenticate themselves, as it was earlier when we created the secure-input and secure-output topics.</p>
</div>
</li>
<li>
<p>Open a new terminal window, navigate to the <code>secure-app</code> folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/security/secure-app</strong></pre>
</div>
</div>
</li>
<li>
<p>To run the <code>kafka-console-producer</code> that we will use to feed some data to the topic <code>secure-input</code> use this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>NS=io.confluent.monitoring.clients.interceptor &amp;&amp; \
kafka-console-producer \
    --bootstrap-server kafka:9091 \
    --topic secure-input \
    --producer.config /etc/kafka/secrets/client_security.properties \
    --producer-property \
    interceptor.classes="${NS}.MonitoringProducerInterceptor"</strong></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>We&#8217;re configuring the producer for monitoring via Control Center through the parameter <code>--producer-property</code></p>
</li>
<li>
<p>We&#8217;re passing the <strong>properties</strong> file with the security settings through the parameter <code>--producer.config</code> to the producer.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>Open another terminal window, navigate to the <code>secure-app</code> folder:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>cd ~/confluent-streams/labs/security/secure-app</strong></pre>
</div>
</div>
</li>
<li>
<p>To list the data produced by our sample application to the topic <code>secure-output</code> use this command:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>NS=io.confluent.monitoring.clients.interceptor &amp;&amp; \
kafka-console-consumer \
    --group secure-console-consumer \
    --bootstrap-server kafka:9091 \
    --topic secure-output \
    --from-beginning \
    --consumer.config /etc/kafka/secrets/client_security.properties \
    --consumer-property \
    interceptor.classes="${NS}.MonitoringConsumerInterceptor"</strong></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="ulist">
<ul>
<li>
<p>We&#8217;re configuring the consumer for monitoring via Control Center through the parameter <code>--consumer-property</code></p>
</li>
<li>
<p>We&#8217;re passing the <strong>properties</strong> file with the security settings through the parameter <code>--consumer.config</code> to the consumer</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</li>
<li>
<p>In the terminal where the producer is running enter a few lines such as:</p>
<div class="literalblock">
<div class="content">
<pre><strong>Kafka is powering the Confluent streaming platform
Real time streaming is exceedingly important
Confluent offers support for Kafka Streaming and ksqlDB
In my company we build Kafka Streams applications</strong></pre>
</div>
</div>
</li>
<li>
<p>Observe how output is generated in the terminal window where the <code>kafka-console-consumer</code> is running. Specifically notice that the order of the output relative to the input might be changed (if you enter values fast enough) due to the fact that we have 3 partitions per topic and ordering is only guaranteed within a partition but not globally per topic!</p>
</li>
<li>
<p><strong>Optional:</strong> Add a lot more data using the <code>kafka-console-producer</code>. For example:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>NS=io.confluent.monitoring.clients.interceptor</strong>
$ <strong>for i in {1 .. 100}; do
        sleep 1
        seq 1000 | kafka-console-producer \
    --bootstrap-server kafka:9091 \
    --topic secure-input \
    --producer.config /etc/kafka/secrets/client_security.properties \
    --producer-property \
    interceptor.classes="${NS}.MonitoringProducerInterceptor"
done</strong></pre>
</div>
</div>
</li>
<li>
<p>Open <strong>Confluent Control Center</strong> at <a href="http://localhost:9021" target="_blank" rel="noopener">http://localhost:9021</a> and go to <strong>Consumers &#8594; secureapp-sample</strong> and you should see something like this:</p>
<div class="paragraph">
<p><span class="image"><img src="./images/security/c3-secure-monitoring.png" alt="c3 secure monitoring"></span></p>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_cleanup_15">Cleanup</h4>
<div class="olist arabic">
<ol class="arabic" start="24">
<li>
<p>Quit both the producer and consumer by pressing <code>Ctrl+C</code>.</p>
</li>
<li>
<p>Quit the sample <strong>Kafka Streams</strong> application with <strong>Run</strong> &#8594; <strong>Stop Debugging</strong>.</p>
</li>
<li>
<p>Shutdown the Kafka cluster:</p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose down -v</strong></pre>
</div>
</div>
</li>
</ol>
</div>
</div>
<div class="sect3">
<h4 id="_conclusion_15">Conclusion</h4>
<div class="paragraph">
<p>In this example we have shown how to run a secure Kafka cluster and then build a <strong>Kafka Streams</strong> application that integrates with this cluster using <strong>SASL</strong> for authentication and SSL for encryption. The application logic was trivial yet that is not the point of this example. The important fact is the integration with the Kafka cluster security settings.</p>
</div>
<div class="paragraph">
<p> 
 
 </p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="./images/stophand.png" alt="stophand" width="200">
</div>
</div>
<div class="paragraph text-center">
<p><strong>STOP HERE. THIS IS THE END OF THE EXERCISE.</strong></p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_running_all_labs_with_docker">Appendix A: Running All Labs with Docker</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="docker-local">Running Labs in Docker for Desktop</h3>
<div class="paragraph">
<p>If you have installed Docker for Desktop on your Mac or Windows 10 Pro machine you are able to complete the course by building and running your applications from the command line.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Increase the memory available to Docker Desktop to a minimum of 6 GiB. See the advanced settings for <a href="https://docs.docker.com/docker-for-mac/#advanced">Docker Desktop for Mac</a>, and <a href="https://docs.docker.com/docker-for-windows/#advanced">Docker Desktop for Windows</a>.</p>
</li>
<li>
<p>Follow the instructions at &#8594; <a href="#preparing-lab">The Lab Environment &amp; Sample Solutions</a> to <code>git clone</code> the source code, in each exercise follow the instructions to launch the cluster containers with <code>docker-compose</code> on your host machine.  The exercise source code will now be on your host machine where you can edit the source code with any editor.</p>
</li>
<li>
<p>Begin the exercises by first opening a bash shell on the tools container. All the command line instructions will work from the tools container.  This container has been preconfigured with all of the tools you use in the exercises, e.g. <code>kafka-topics</code> and <code>python.</code></p>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose exec tools bash</strong>
bash-4.4#</pre>
</div>
</div>
</li>
<li>
<p>At the time of writing, the Python prerequisite <code>confluent_kafka</code> won&#8217;t install with the version of Python 3 available in the tools container. You should use <code>pip</code> and <code>python</code> rather than <code>pip3</code> and <code>python3</code>.</p>
</li>
<li>
<p>At the time of writing, Maven is not installed in the tools container, so the optional maven exercise would require a hefty 300 MB download with <code>apt-get install maven</code>. It might be best to skip this optional exercise and simply use it as reference if you use Maven in your day-to-day work.</p>
</li>
<li>
<p>Anywhere you are instructed to open additional terminal windows you can <code>exec</code> additional bash shells on the tools container with the same command as above on your host machine.</p>
</li>
<li>
<p>Any subsequent <code>docker</code> or <code>docker-compose</code> instructions should be run on your host machine.</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_running_the_exercise_applications">Running the Exercise Applications</h3>
<div class="paragraph">
<p>From the <code>tools</code> container you can use command line alternatives to the VS Code steps used in the instructions.  Complete the exercise code with an editor on your host machine, then use the following command line instructions to build and run the applications from the exercise directory.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>For Java applications: <code>./gradlew run</code></p>
</li>
<li>
<p>For Python applications: <code>python main.py</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Where you are instructed to use <strong>Run</strong> &#8594; <strong>Stop Debugging</strong> in VS code, use <code>Ctrl+C</code> to end the running exercise.</p>
</div>
<div class="paragraph">
<p>Our <code>docker-compose.yml</code> file sets the working directory inside the tools container to the <code>~/confluent-streams directory</code> Be sure to change the working directory to each exercise directory as stated in each exercise instructions.</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose exec tools bash</strong>
bash-4.4# <strong>pwd</strong>
/root/confluent-streams</pre>
</div>
</div>
<div class="paragraph">
<p>To build and run the <strong>Anatomy of a Kafka Streams App</strong> exercise. First make the source code updates to <code>~/confluent-streams/labs/streams-writing/gradle-sample/src/main/java/streams/MapSample.java</code> on your host machine.  Then enter into the bash shell on your tools container:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>bash-4.4# <strong>cd labs/streams-writing/gradle-sample</strong>
bash-4.4# <strong>./gradlew run</strong></pre>
</div>
</div>
<div class="paragraph">
<p>In the <strong>Monitoring Kafka Streams Applications</strong> exercise, you must use <code>jconsole</code> on port 4444 on the host to view JMX metrics. If <code>jconsole</code> is not already installed on your host system, install using the password <code>training</code>:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>sudo apt install -y openjdk-11-jdk</strong></pre>
</div>
</div>
<div class="paragraph">
<p>Run a new tools container with a port mapping to expose JMX metrics to the host:</p>
</div>
<div class="listingblock">
<div class="content">
<pre>$ <strong>docker-compose run -p 4444:4444 tools</strong>
bash-4.4# <strong>cd jmx-sample &amp;&amp; ./gradlew run</strong></pre>
</div>
</div>
<div class="paragraph">
<p>The application will now expose metrics to port 4444 in the container, which is mapped to port 4444 on the host. Running <code>jconsole</code> on the host on port 4444 will now pick up the metrics exposed by the application.</p>
</div>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 7.0.0-v1.0.1<br>
Last updated 2022-05-20 11:55:04 UTC
</div>
</div>
</body>
</html>